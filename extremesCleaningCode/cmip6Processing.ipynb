{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianreed/opt/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "\n",
    "import collections\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# import geopandas as gpd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import rasterio\n",
    "\n",
    "from difflib import get_close_matches\n",
    "\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import xclim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq = pd.read_csv(\"../../data/companyData/justHQs.csv\") \n",
    "\n",
    "\n",
    "min_lat = hq.latitude.min()\n",
    "max_lat = hq.latitude.max()\n",
    "\n",
    "min_lon = hq.longitude.min()\n",
    "max_lon = hq.longitude.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"baselineName = '../../../../../../../Volumes/backup2/dissData/cmip6Data/hist/mirocHQs_198199.csv'\\nbaseline = pd.read_csv(baselineName)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''baselineName = '../../../../../../../Volumes/backup2/dissData/cmip6Data/hist/mirocHQs_198199.csv'\n",
    "baseline = pd.read_csv(baselineName)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data\n",
    "## *this is how we process the cmip6Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://kpegion.github.io/Pangeo-at-AOES/examples/advanced-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = 'pr'\n",
    "scenario = 'ssp585'\n",
    "\n",
    "file1 = '../../../../../../../Volumes/backup2/dissData/cmip6Data/proj/' + weather + '/' + weather + '_day_MIROC6_' + scenario + '_r*i1p1f1_gn_20150101-20241231.nc'\n",
    "file2 = '../../../../../../../Volumes/backup2/dissData/cmip6Data/proj/' + weather + '/' + weather + '_day_MIROC6_' + scenario + '_r*i1p1f1_gn_20250101-20341231.nc'\n",
    "file3 = '../../../../../../../Volumes/backup2/dissData/cmip6Data/proj/' + weather + '/' + weather + '_day_MIROC6_' + scenario + '_r*i1p1f1_gn_20350101-20441231.nc'\n",
    "\n",
    "data1 = xr.open_mfdataset(file1,concat_dim=['ensemble'],combine='nested',decode_times=True)\n",
    "data2 = xr.open_mfdataset(file2,concat_dim=['ensemble'],combine='nested',decode_times=True)\n",
    "data3 = xr.open_mfdataset(file3,concat_dim=['ensemble'],combine='nested',decode_times=True)\n",
    "\n",
    "data = xr.combine_by_coords([data1,data2,data3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFutureData(weather,scenario):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # weather = 'pr'\n",
    "\n",
    "\n",
    "    file1 = '../../../../../../../Volumes/backup2/dissData/cmip6Data/proj/' + weather + '/' + weather + '_day_MIROC6_' + scenario + '_r*i1p1f1_gn_20150101-20241231.nc'\n",
    "    file2 = '../../../../../../../Volumes/backup2/dissData/cmip6Data/proj/' + weather + '/' + weather + '_day_MIROC6_' + scenario + '_r*i1p1f1_gn_20250101-20341231.nc'\n",
    "    file3 = '../../../../../../../Volumes/backup2/dissData/cmip6Data/proj/' + weather + '/' + weather + '_day_MIROC6_' + scenario + '_r*i1p1f1_gn_20350101-20441231.nc'\n",
    "\n",
    "    data1 = xr.open_mfdataset(file1,concat_dim=['ensemble'],combine='nested',decode_times=True)\n",
    "    data2 = xr.open_mfdataset(file2,concat_dim=['ensemble'],combine='nested',decode_times=True)\n",
    "    data3 = xr.open_mfdataset(file3,concat_dim=['ensemble'],combine='nested',decode_times=True)\n",
    "\n",
    "    data = xr.combine_by_coords([data1,data2,data3])\n",
    "\n",
    "    data.coords['lon'] = (data.coords['lon'] + 180) % 360 - 180\n",
    "    data = data.sortby(data.lon)\n",
    "\n",
    "    data = data.sel(lat=slice(min_lat,max_lat), \n",
    "                    lon=slice(min_lon,max_lon),\n",
    "                   time=slice('2019-01-01','2039-12-31'))\n",
    "\n",
    "    data['time'] = pd.to_datetime(data.time.values.astype(str))\n",
    "    \n",
    "    print(\"starting to load\")\n",
    "    start = time.time()\n",
    "    # data = data.compute(workers = 100)\n",
    "\n",
    "    time.time() - start\n",
    "\n",
    "    # , ,combine = 'by_coords')   #  nc.Dataset(file)\n",
    "\n",
    "    print(\"ending the load: \", time.time() - start)\n",
    "\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.81979489326477\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "file = '../../../../../../../Volumes/backup2/dissData/cmip6Data/hist/pr/pr_day_MIROC6_historical_r*i1p1f1_gn_19800101-19891231.nc'\n",
    "data1 = xr.open_mfdataset(file,concat_dim=['ensemble'],combine='nested',decode_times=True)\n",
    "\n",
    "\n",
    "\n",
    "# , ,combine = 'by_coords')   #  nc.Dataset(file)\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.12129926681519\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "file = '../../../../../../../Volumes/backup2/dissData/cmip6Data/hist/pr/pr_day_MIROC6_historical_r*i1p1f1_gn_19900101-19991231.nc'\n",
    "# file = '../../../../../../../Volumes/backup2/dissData/cmip6Data/hist/pr/pr_day_MIROC6_historical_r*i1p1f1_gn_19800101-19891231.nc'\n",
    "# data = xr.open_mfdataset(file,concat_dim=['ensemble'],combine='nested',decode_times=True)\n",
    "data2 = xr.open_mfdataset(file,concat_dim=['ensemble'],combine='nested',decode_times=True)\n",
    "# , ,combine = 'by_coords')   #  nc.Dataset(file)\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this now for some reason seems to work\n",
    "'''from xclim import ensembles\n",
    "import glob\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import xclim as xc\n",
    "ens = ensembles.create_ensemble(glob.glob(file)).load()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.66204476356506\n",
      "46.66490173339844\n",
      "46.67260694503784\n"
     ]
    }
   ],
   "source": [
    "data = xr.combine_by_coords([data1,data2])\n",
    "\n",
    "print(time.time() - start)\n",
    "data.coords['lon'] = (data.coords['lon'] + 180) % 360 - 180\n",
    "print(time.time() - start)\n",
    "data = data.sortby(data.lon)\n",
    "print(time.time() - start)\n",
    "\n",
    "data = data.sel(lat=slice(min_lat,max_lat), \n",
    "                lon=slice(min_lon,max_lon),\n",
    "               time=slice('1981-01-01','1999-12-31'))\n",
    "\n",
    "\n",
    "data['time'] = pd.to_datetime(data.time.values.astype(str))\n",
    "\n",
    "start = time.time()\n",
    "data = data.compute(workers = 100)\n",
    "\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summaries\n",
    "Summarize some of the data here.\n",
    "\n",
    "### average weather by coords across ensemble and across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4700090885162354\n",
      "7.061110019683838\n"
     ]
    }
   ],
   "source": [
    "start       = time.time()\n",
    "\n",
    "\n",
    "overall     = data.mean(dim = ['ensemble','lat','lon'])\n",
    "overallMean = overall.pr.values\n",
    "print(time.time() - start)\n",
    "\n",
    "\n",
    "overallStd  = data.std(dim = ['ensemble','lat','lon'])\n",
    "overallStd  = overallStd.pr.values\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile =  '../../data/companyData/miroc_historicalMean.pkl'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pkl.dump(overallMean, pickle_file)\n",
    "    \n",
    "outfile =  '../../data/companyData/miroc_historicalStdDev.pkl'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pkl.dump(overallStd, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''start       = time.time()\n",
    "\n",
    "\n",
    "byCoord = data.mean(dim = ['ensemble','time'])\n",
    "ensembleMeanByCoord = byCoord.pr.values\n",
    "\n",
    "time.time() - start'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95th percentile by ensemble\n",
    "find percentile by quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHistData(weather):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # weather = 'pr'\n",
    "\n",
    "\n",
    "    file1 = '../../../../../../../Volumes/backup2/dissData/cmip6Data/hist/' + weather + '/' + weather + '_day_MIROC6_historical_r*i1p1f1_gn_19800101-19891231.nc'\n",
    "    file2 = '../../../../../../../Volumes/backup2/dissData/cmip6Data/hist/' + weather + '/' + weather + '_day_MIROC6_historical_r*i1p1f1_gn_19900101-19991231.nc'\n",
    "\n",
    "\n",
    "    data1 = xr.open_mfdataset(file1,concat_dim=['ensemble'],combine='nested',decode_times=True)\n",
    "    data2 = xr.open_mfdataset(file2,concat_dim=['ensemble'],combine='nested',decode_times=True)\n",
    "\n",
    "    data = xr.combine_by_coords([data1,data2])\n",
    "\n",
    "    data.coords['lon'] = (data.coords['lon'] + 180) % 360 - 180\n",
    "    data = data.sortby(data.lon)\n",
    "\n",
    "    data = data.sel(lat=slice(min_lat,max_lat), \n",
    "                    lon=slice(min_lon,max_lon),\n",
    "                   time=slice('1981-01-01','1999-12-31'))\n",
    "\n",
    "    data['time'] = pd.to_datetime(data.time.values.astype(str))\n",
    "    \n",
    "    print(\"starting to load\")\n",
    "    start = time.time()\n",
    "    data = data.compute(workers = 100)\n",
    "\n",
    "    time.time() - start\n",
    "\n",
    "    # , ,combine = 'by_coords')   #  nc.Dataset(file)\n",
    "\n",
    "    print(\"ending the load: \", time.time() - start)\n",
    "\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (bnds: 2, ensemble: 50, lat: 25, lon: 64, time: 1714)\n",
       "Coordinates:\n",
       "  * lat        (lat) float64 21.71 23.11 24.51 25.91 ... 51.13 52.53 53.93 55.33\n",
       "  * lon        (lon) float64 -157.5 -156.1 -154.7 ... -71.72 -70.31 -68.91\n",
       "  * time       (time) datetime64[ns] 1981-01-01T12:00:00 ... 1999-03-31T12:00:00\n",
       "    quarter    (time) int64 1 1 1 1 1 1 1 1 1 1 1 1 ... 1 1 1 1 1 1 1 1 1 1 1 1\n",
       "Dimensions without coordinates: bnds, ensemble\n",
       "Data variables:\n",
       "    time_bnds  (ensemble, time, bnds) datetime64[ns] 1981-01-01 ... 1999-04-01\n",
       "    lat_bnds   (time, ensemble, lat, bnds) float64 21.01 22.41 ... 54.63 56.03\n",
       "    lon_bnds   (time, ensemble, lon, bnds) float64 201.8 203.2 ... 290.4 291.8\n",
       "    pr         (ensemble, time, lat, lon) float32 1.5386958e-06 ... 4.6878404e-07"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df.time.dt.quarter == 1,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPercentileByQuarter(quarter, df):\n",
    "    perQuarter     = df.where(df.time.dt.quarter == quarter,drop=True)\n",
    "    percPerQuarter = perQuarter.quantile(0.95, dim = ('time'))\n",
    "    \n",
    "    return(percPerQuarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = getHistData('pr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_pr = getPercentileByQuarter(1,pr)\n",
    "q2_pr = getPercentileByQuarter(2,pr)\n",
    "q3_pr = getPercentileByQuarter(3,pr)\n",
    "q4_pr = getPercentileByQuarter(4,pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'ensemble' (ensemble: 21)>\n",
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20])\n",
       "Dimensions without coordinates: ensemble"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_future.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_future = getFutureData('pr','ssp585')\n",
    "\n",
    "# subtract them and find out where > 0, per quarter\n",
    "pr_future.where(pr_future.time.dt.quarter == 1,drop=True) - q1_pr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tasmax - future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to load\n",
      "ending the load:  1.1920928955078125e-06\n"
     ]
    }
   ],
   "source": [
    "tasmax_future = getFutureData('tasmax','ssp585')\n",
    "tasmax_future['tasmax_F'] = 1.8*(tasmax_future['tasmax']-273) + 32\n",
    "\n",
    "\n",
    "# subtract them and find out where > 0, per quarter\n",
    "# tasmax_future.where(pr_future.time.dt.quarter == 1,drop=True) - q1_pr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasmax_future['tasmax_F'] = 1.8*(tasmax_future['tasmax']-273) + 32\n",
    "tasmax_future['extreme']  = 1*(tasmax_future['tasmax_F'] > 90)\n",
    "futureTempExtremes        = tasmax_future.groupby(pr_future.time.dt.quarter).mean(dim = ['ensemble','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 51.20 kB </td> <td> 12.80 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (4, 25, 64) </td> <td> (1, 25, 64) </td></tr>\n",
       "    <tr><th> Count </th><td> 3756 Tasks </td><td> 4 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"201\" height=\"117\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"31\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"46\" x2=\"31\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"46\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"15\" y1=\"5\" x2=\"15\" y2=\"52\" />\n",
       "  <line x1=\"20\" y1=\"10\" x2=\"20\" y2=\"57\" />\n",
       "  <line x1=\"25\" y1=\"15\" x2=\"25\" y2=\"62\" />\n",
       "  <line x1=\"31\" y1=\"21\" x2=\"31\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.000000,0.000000 31.093851,21.093851 31.093851,67.968851 10.000000,46.875000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"130\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"15\" y1=\"5\" x2=\"135\" y2=\"5\" />\n",
       "  <line x1=\"20\" y1=\"10\" x2=\"140\" y2=\"10\" />\n",
       "  <line x1=\"25\" y1=\"15\" x2=\"145\" y2=\"15\" />\n",
       "  <line x1=\"31\" y1=\"21\" x2=\"151\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"31\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"130\" y1=\"0\" x2=\"151\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.000000,0.000000 130.000000,0.000000 151.093851,21.093851 31.093851,21.093851\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"31\" y1=\"21\" x2=\"151\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"31\" y1=\"67\" x2=\"151\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"31\" y1=\"21\" x2=\"31\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"151\" y1=\"21\" x2=\"151\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"31.093851,21.093851 151.093851,21.093851 151.093851,67.968851 31.093851,67.968851\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"91.093851\" y=\"87.968851\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >64</text>\n",
       "  <text x=\"171.093851\" y=\"44.531351\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,171.093851,44.531351)\">25</text>\n",
       "  <text x=\"10.546926\" y=\"77.421926\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,10.546926,77.421926)\">4</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<concatenate, shape=(4, 25, 64), dtype=float64, chunksize=(1, 25, 64), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "futureTempExtremes.extreme.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.77485179901123"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start                         = time.time()\n",
    "percentileByQuarterByEnsemble = pr.groupby(pr.time.dt.quarter).quantile(0.95, dim = ('time'))\n",
    "# percentileByEnsemble        = data.quantile(0.95, dim = ('time'))\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatives: can do percentiles on a per-ensemble basis, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile =  '../../data/companyData/miroc_historical95s.pkl'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pkl.dump(percentileByEnsemble, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (bnds: 2, ensemble: 50, lat: 25, lon: 64)\n",
       "Coordinates:\n",
       "  * lat        (lat) float64 21.71 23.11 24.51 25.91 ... 51.13 52.53 53.93 55.33\n",
       "  * lon        (lon) float64 -157.5 -156.1 -154.7 ... -71.72 -70.31 -68.91\n",
       "    quantile   float64 0.95\n",
       "Dimensions without coordinates: bnds, ensemble\n",
       "Data variables:\n",
       "    time_bnds  (ensemble, bnds) datetime64[ns] 1999-01-18T02:23:59.999999952 ... 1999-01-19T02:23:59.999999952\n",
       "    lat_bnds   (ensemble, lat, bnds) float64 21.01 22.41 22.41 ... 54.63 56.03\n",
       "    lon_bnds   (ensemble, lon, bnds) float64 201.8 203.2 203.2 ... 290.4 291.8\n",
       "    pr         (ensemble, lat, lon) float64 6.968e-05 7.182e-05 ... 0.0001168"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentileByEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (bnds: 2, ensemble: 50, lat: 25, lon: 64, time: 6939)\n",
       "Coordinates:\n",
       "  * lat        (lat) float64 21.71 23.11 24.51 25.91 ... 51.13 52.53 53.93 55.33\n",
       "  * lon        (lon) float64 -157.5 -156.1 -154.7 ... -71.72 -70.31 -68.91\n",
       "  * time       (time) datetime64[ns] 1981-01-01T12:00:00 ... 1999-12-31T12:00:00\n",
       "    quantile   float64 0.95\n",
       "Dimensions without coordinates: bnds, ensemble\n",
       "Data variables:\n",
       "    time_bnds  (ensemble, time, bnds) timedelta64[ns] -6592 days +21:36:00.000000048 ... 346 days 21:36:00.000000048\n",
       "    lat_bnds   (time, ensemble, lat, bnds) float64 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
       "    lon_bnds   (time, ensemble, lon, bnds) float64 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
       "    pr         (ensemble, time, lat, lon) float64 -6.814e-05 ... -0.0001129"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data - percentileByEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../../data/companyData/miroc_historical95s.pkl','rb')\n",
    "percentileByEnsemble = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentileByEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and xarray and all that\n",
    "# https://www.earthinversion.com/utilities/reading-NetCDF4-data-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ensemble == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://metview.readthedocs.io/en/latest/examples/ens_mean_spread_xarray.html\n",
    "# data.mean(dim = 'ensemble')\n",
    "overall = data.mean(dim = ['lat','lon'])\n",
    "overall.pr.plot(hue = 'ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-dark\")\n",
    "plt.rcParams[\"figure.figsize\"] = (13, 5)\n",
    "ens.tas.plot(hue=\"realization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes the ensemble mean and then the mean over time\n",
    "data_emean = data.mean()# .mean() \n",
    "print(data_emean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import xclim as xc\n",
    "\n",
    "# Set display to HTML sytle (for fancy output)\n",
    "xr.set_options(display_style=\"html\", display_width=50)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from xclim import ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glob.glob('../../../../../../../Volumes/backup2/dissData/cmip6Data/hist/pr/pr_day_MIROC6_historical_r1*i1p1f1_gn_19800101-19891231.nc'))\n",
    "datasets = glob.glob('../../../../../../../Volumes/backup2/dissData/cmip6Data/hist/pr/pr_day_MIROC6_historical_r1*i1p1f1_gn_19800101-19891231.nc')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = \n",
    "# data = xr.open_mfdataset(file,concat_dim='ensemble',combine='nested',decode_times=True)\n",
    "\n",
    "ens = ensembles.create_ensemble(datasets, mf_flag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoHQ = gpd.GeoDataFrame(\n",
    "    allIG,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        allIG[\"longitude\"],\n",
    "        allIG[\"latitude\"],\n",
    "    ),\n",
    "    crs={\"init\":\"EPSG:4326\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_point = data[\"pr\"].sel(lat=50, lon=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test.temperature[test.year == 2010]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go through and make this iterable for the entire list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "data.pr.sel(allIG.iloc[0,:], method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqPost2010 = geoHQ[geoHQ.archive_version_year > 2010].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "tempsList = list()\n",
    "for i in range(0,hqPost2010.shape[0]):\n",
    "    nearbyTemps = temps.sel(hqPost2010.loc[i,['latitude','longitude']], method='nearest')\n",
    "    \n",
    "    tempsList.append(list(np.array(nearbyTemps.temperature[nearbyTemps.year == hqPost2010.archive_version_year[i]])))\n",
    "\n",
    "    if (i%100 == 0):\n",
    "        print(i)\n",
    "    \n",
    "print(time.time() - start)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile =  'data/hqDailyTemperatures.pkl'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pickle.dump(tempsList, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's do the percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselineName = '../../../../../../../Volumes/backup2/dissData/cmip6Data/hist/mirocHQs_198199.csv'\n",
    "baseline = pd.read_csv(baselineName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.s.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline[baseline.variable == 'tasmax_hist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantsPrecip   = baseline[baseline.variable == 'pr_hist'][['zipcode','variable','quarter','value']].\\\n",
    "    groupby(['zipcode','variable','quarter']).quantile(.95).reset_index()\n",
    "quantsPrecip.rename(columns = {'value': 'quantCutoff'}, inplace = True)\n",
    "quantsPrecip['daysExtreme'] = 4.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantsPrecip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find number of days above 90F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = baseline[baseline.variable == 'tasmax_hist'].drop(columns = {'s','year'})\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantsTemp = baseline[baseline.variable == 'tasmax_hist'].drop(columns = {'s','year'})\n",
    "quantsTemp['daysExtreme'] = 1*(quantsTemp.value > 305.3)\n",
    "quantsTemp = quantsTemp.groupby(['zipcode', 'variable', 'quarter']).median().reset_index()\n",
    "quantsTemp.rename(columns = {'value': 'quantCutoff'}, inplace = True)\n",
    "\n",
    "quantsTemp.daysExtreme = quantsTemp.daysExtreme*90\n",
    "quantsTemp.quantCutoff = 305.3\n",
    "\n",
    "\n",
    "quantsTemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantsPrecip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quants = quantsPrecip.append(quantsTemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And apply them to the next period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPeriodName = '../../../../../../../Volumes/backup2/dissData/cmip6Data/proj/mirocHQs_202040.csv'\n",
    "nextPeriod = pd.read_csv(nextPeriodName)# .drop(columns = ['s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPeriod.shape[0]/len(nextPeriod.zipcode.unique())/365/18/2/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPeriod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quants.daysExtreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPeriod = nextPeriod.merge(quants)\n",
    "nextPeriod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPeriod['extreme'] = 1*(nextPeriod.value > nextPeriod.quantCutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nextPeriod.to_csv(\"../../data/companyData/nextPeriodExtremes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPeriod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPeriod.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPeriod['yearQuarter'] = nextPeriod.year.astype('int64') + nextPeriod.quarter.str[1:2].astype('int64')/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPeriod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPeriod[['zipcode']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPeriod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPeriod[nextPeriod.variable == 'pr_hist'].extreme.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterlyExtremes = nextPeriod[['variable','s','year','quarter','yearQuarter','zipcode','extreme']].\\\n",
    "    groupby(['variable','s','year','quarter','yearQuarter','zipcode']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipLats = pd.read_csv(\"../../data/companyData/quarterlyExtremesMean.csv\")[['zipcode','latitude','longitude']].\\\n",
    "    groupby(['zipcode']).mean().reset_index()\n",
    "\n",
    "quarterlyExtremes = quarterlyExtremes.merge(zipLats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterlyExtremes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterlyExtremes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterlyExtremes.to_csv(\"../../data/companyData/quarterlyExtremesAll.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterlyExtremesMean = quarterlyExtremes[['variable','quarter','zipcode','latitude','longitude','extreme']].\\\n",
    "    groupby(['variable','quarter','zipcode','latitude','longitude']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterlyExtremesMean[quarterlyExtremesMean.variable == 'tasmax_hist'].extreme.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterlyExtremesMean[quarterlyExtremesMean.variable == 'pr_hist'].extreme.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterlyExtremesMean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterlyExtremesMean = quarterlyExtremesMean.merge(quants[['zipcode','variable','quarter','daysExtreme']])\n",
    "quarterlyExtremesMean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterlyExtremesMean.to_csv(\"../../data/companyData/quarterlyExtremesMean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipLats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterlyExtremesMean.groupby(['variable']).extreme.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might have multiple records from within the same zipcode\n",
    "quarterlyExtremesMean = quarterlyExtremes[['variable','quarter','zipcode','extreme']].\\\n",
    "    groupby(['variable','quarter','zipcode']).mean().reset_index()\n",
    "\n",
    "# .groupby(['variable','quarter','zipcode']).mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterlyExtremesMean.groupby('variable').extreme.hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(quarterlyExtremesMean[quarterlyExtremesMean.variable == 'tasmax_hist'].extreme - 4.5).hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(quarterlyExtremesMean[quarterlyExtremesMean.variable == 'pr_hist'].extreme - 4.5).hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(quarterlyExtremesMean[quarterlyExtremesMean.variable == 'pr_hist'].extreme > 4.5)/quarterlyExtremesMean[quarterlyExtremesMean.variable == 'pr_hist'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(quarterlyExtremesMean[quarterlyExtremesMean.variable == 'tasmax_hist'].\\\n",
    "    extreme > 4.5)/quarterlyExtremesMean[quarterlyExtremesMean.variable == 'tasmax_hist'].shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
