{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "\n",
    "import collections\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import rasterio\n",
    "\n",
    "import spacy\n",
    "  \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Full IG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../../data/companyData/infogroup2010s.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(file, assume_missing=True, \n",
    "                 dtype={'parent_number': 'object','parent_employee_size_code': 'object',\n",
    "                       'parent_sales_volume_code': 'object',\n",
    "                       'abi': 'object','zipcode': 'object'}, low_memory = False)\n",
    "df = df[df.business_status_code == 1.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq = df[['abi','ticker','company','archive_version_year','state','city',\n",
    "         'address_line_1','zipcode',\n",
    "         'latitude','longitude']].drop_duplicates().compute(num_workers = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly = hq[['abi','company']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly.company.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hq.shape,hqsOnly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the abi numbers seem to be duplicated; it looks like they might be primarily for different government agencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly.company.value_counts()[hqsOnly.company.value_counts() > 10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toDiscard = hqsOnly.company.value_counts()[hqsOnly.company.value_counts() > 1].index\n",
    "for company in toDiscard:\n",
    "    print(company)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toDiscard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly = hqsOnly[~hqsOnly.company.isin(toDiscard)]\n",
    "hq      = hq[~hq.company.isin(toDiscard)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a unique record of every company - hq here. Some of these may well be duplicate entries for a given company, for the cases in which we have a company that has multiple hq.\n",
    "\n",
    "Let's stash it so that we don't have to go through the above ^^ again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly.to_csv(\"../../data/ig2010s_uniqueHQs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq.to_csv(\"../../data/ig2010s_uniqueHQs_multLocations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly     = pd.read_csv(\"../../data/ig2010s_uniqueHQs.csv\").drop(columns = {'Unnamed: 0'})\n",
    "hqsWithYear = pd.read_csv(\"../../data/ig2010s_uniqueHQs_multLocations.csv\",dtype={'zipcode': 'object'})[['abi','company',\n",
    "                                                                             'archive_version_year',\n",
    "                                                                             'state','city','zipcode','address_line_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsWithYear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsWithYear = hqsWithYear[hqsWithYear.archive_version_year <= 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsWithYear['zipcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsWithYear['last_year'] = hqsWithYear.groupby(['abi'])['archive_version_year'].transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hqsWithYear.shape)\n",
    "\n",
    "lastHQs = hqsWithYear[hqsWithYear.archive_version_year == hqsWithYear.last_year][['abi','company','state','city','zipcode','address_line_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastHQs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Compustat Data\n",
    "\n",
    "First filter down to the companies for whom we have the supply chain information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_links = pd.read_csv(\"../../data/companyData/compustatSCLinked.csv\")\n",
    "\n",
    "c_links['year'] = c_links.srcdate.astype('str').str.slice(0,4).astype('int64')\n",
    "\n",
    "c_links = c_links[c_links.year > 2009]\n",
    "\n",
    "relevant_gvkeys = c_links.gvkey.append(c_links.cgvkey).drop_duplicates()\n",
    "\n",
    "print(c_links.head(),relevant_gvkeys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the company dataset and check.\n",
    "\n",
    "The legal name and the given name are slightly different, but basically the same modulo punctuation and case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_addresses = pd.read_csv(\"../../data/companyData/compustatAddresses.csv\", \n",
    "                dtype={'parent_number': 'object'})[['fyear',\n",
    "                'gvkey',\n",
    "                'conm',\n",
    "                'add1',\n",
    "                'city',\n",
    "                'state',\n",
    "                'idbflag',\n",
    "                'addzip',\n",
    "               'naics']].drop_duplicates().rename(columns = {'fyear': 'year'})\n",
    "c_addresses = c_addresses[(c_addresses.year > 2009) & (c_addresses.year < 2020)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_addresses.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset this to focus on firms in: ag, mining, construction, manufacturing, wholesale and retail, and transportation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_addresses = c_addresses[(c_addresses.naics.astype('str').str.slice(0,2).isin(['11','21','22','23','31','32',\n",
    "                                                         '33','42','44','45','48','49']))]\n",
    "\n",
    "chq = c_addresses[['gvkey','conm','add1','city','state','addzip','idbflag']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're starting with the compustat north america dataset. Not all of the HQs are in North America, so we can filter some of the information down to match with Infogroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq.idbflag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canadian = ['ON', 'AB','QC', 'BC', 'NS', 'NF', 'SK', 'MB', 'NB']\n",
    "\n",
    "chq.state.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq = chq[~(chq.state.isin(canadian)) & ~chq.state.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq.addzip.str.len().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq['addzip'] = chq.addzip.astype('str').str.slice(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq[chq.idbflag == 'D'].addzip.str.len().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq[chq.idbflag == \"B\"].addzip.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chq.head(),chq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq.rename(columns = {'conm': 'company','addzip': 'cstatZipcode'},inplace = True)\n",
    "chq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only two of these company names appear 2x, which is good. There are ~20,000 companies in this sample.\n",
    "\n",
    "Let's go through a little bit of a process here:\n",
    "- Find the exact matches.\n",
    "- Get a similarity measure between ; ideally something vectorized / something in matrix math.\n",
    "- Find the top 10 matches for the remaining ones.\n",
    "- Do some mix and match and see if there's any threshold at which matches become similar ``enough'' to say this is okay and good to go.\n",
    "\n",
    "\n",
    "We might be able to use the fact that all of the addresses should be the same after some given point, as the compustat addresses are only the most recent ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few different ways to match these up.\n",
    "\n",
    "First, let's find the exact matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a generic cleaning function that strips out all company names, any punctuation in the name, and makes everything lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = text.\\\n",
    "    replace(\" CORP\",\"\").replace(\" CO\",\"\").replace(\" INC\",\"\").\\\n",
    "    replace(\" LTD\",\"\").replace(\" -CL A\",\"\").\\\n",
    "    replace(\" -LP\",\"\").replace(\" LP\",\"\").\\\n",
    "    replace(\"-OLD\",\"\").replace(\" LLC\",\"\").\\\n",
    "    replace(\" -CL B\",\"\").replace(\" -CL i\",\"\").replace(\" -CL\",\"\").\\\n",
    "    replace(\"-REDH\",\"\").replace(\" CP\",\"\").\\\n",
    "    replace(\"-ADR\",\"\").replace(\" PLC\",\"\").lower().replace(r'[^\\w\\s]+', '').\\\n",
    "    replace('-lp','').replace('-spn','').replace('hldg','').replace(' intl','').\\\n",
    "    replace('holdings','').replace('holding','').replace('prtnr','').replace('group','')\n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq['company']               = list(map(cleanText, chq.company))\n",
    "lastHQs['company']           = list(map(cleanText, lastHQs.company))\n",
    "\n",
    "chq.rename(columns = {'city': 'cstatCity',\n",
    "                     'state': 'cstatState',\n",
    "                     'add1': 'cstatadd1'}, inplace = True)\n",
    "\n",
    "chq['cstatCity']  = chq.cstatCity.str.lower()\n",
    "chq['cstatState'] = chq.cstatState.str.lower()\n",
    "chq['cstatadd1']  = chq.cstatadd1.str.lower()\n",
    "\n",
    "lastHQs['city']            = lastHQs.city.str.lower()\n",
    "lastHQs['state']           = lastHQs.state.str.lower()\n",
    "lastHQs['address_line_1']  = lastHQs.address_line_1.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAICS names do not match up between compustat and infogroup so they're not helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastHQs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match on company name directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameMerge = chq.merge(lastHQs)\n",
    "nameMerge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameMerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(nameMerge.cstatState == nameMerge.state)/nameMerge.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(nameMerge.cstatZipcode.str.slice(0,5) == nameMerge.zipcode.str.slice(0,5))/nameMerge.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(nameMerge.cstatZipcode.str.slice(0,1) == nameMerge.zipcode.str.slice(0,1))/nameMerge.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameMerge[nameMerge.cstatCity != nameMerge.city][50:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now focus down onto the companies that have not been matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqUnmatched = chq[~chq.company.isin(nameMerge.company)].reset_index()\n",
    "chqUnmatched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igUnmatched  = lastHQs[~lastHQs.company.isin(nameMerge.company)].reset_index()\n",
    "igUnmatched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastHQs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two distance measures here. Look at top 5 matches and pull the distance measure and matches as well.\n",
    "\n",
    "### Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as levenshtein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyArrayCStat = []\n",
    "\n",
    "company = chqUnmatched.company[0]\n",
    "start = time.time()\n",
    "for company in chqUnmatched.company:\n",
    "    thisCompany = []\n",
    "    for ig in igUnmatched.company:\n",
    "        thisCompany.append(levenshtein_distance(company,ig))\n",
    "    \n",
    "    companyArrayCStat.append([thisCompany])\n",
    "\n",
    "allLD = np.concatenate(companyArrayCStat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igUnmatched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "singleLargestLV   = (allLD).argsort(axis=-1)[:, :1]\n",
    "largestElementsLV = (allLD).argsort(axis=-1)[:, :n]\n",
    "\n",
    "\n",
    "companyMatches = pd.DataFrame()\n",
    "companyMatches['cstatCompanies'] = chqUnmatched.company\n",
    "companyMatches['cstatadd1']      = chqUnmatched.cstatadd1\n",
    "companyMatches['cstatCity']      = chqUnmatched.cstatCity\n",
    "companyMatches['cstatState']     = chqUnmatched.cstatState\n",
    "companyMatches['cstatZip']       = chqUnmatched.cstatZipcode\n",
    "\n",
    "for i in range(0,companyMatches.shape[0]):\n",
    "    # print(list(np.array(hq.company)[largestElements[i]]))\n",
    "    companyMatches.at[i,'misspelling']         = allLD[i,:][singleLargestLV[i]]\n",
    "    companyMatches.at[i,'levCompany']          = igUnmatched.company[singleLargestLV[i]].iloc[0]\n",
    "    \n",
    "    companyMatches.at[i,'closestMatchIG_add']      = igUnmatched.address_line_1[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches.at[i,'closestMatchIG_city']     = igUnmatched.city[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches.at[i,'closestMatchIG_zipcode']  = igUnmatched.zipcode[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches.at[i,'closestMatchIG_state']    = igUnmatched.state[singleLargestLV[i]].iloc[0]\n",
    "\n",
    "    \n",
    "    # companyMatches.at[i,'closestMatchIG']      = np.array(igUnmatched.company)[largestElementsLV[i]]\n",
    "    # companyMatches.at[i,'LevSim']              = np.array(allLD[i,:][largestElementsLV[i]], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the embeddings and the cosine similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatrix(companyEmbeddings):\n",
    "    companyArray = []\n",
    "    \n",
    "    for companies in companyEmbeddings:\n",
    "        companyArray.append([companies.vector])\n",
    "    \n",
    "    companyArray = np.concatenate(companyArray)\n",
    "    \n",
    "    return(companyArray)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile =  '../../data/allCompaniesIG_embeddings.pkl'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pkl.dump(allCompaniesIG, pickle_file)\n",
    "    \n",
    "outfile =  '../../data/allCompaniesCStat_embeddings.pkl'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pkl.dump(allCompaniesCStat, pickle_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqUnmatchedList = list(map(nlp, chqUnmatched.company))\n",
    "allCompaniesIG = list(map(nlp, igUnmatched.company))\n",
    "\n",
    "\n",
    "cstat = getMatrix(chqUnmatchedList)\n",
    "ig    = getMatrix(allCompaniesIG)\n",
    "\n",
    "allSimilarities = cosine_similarity(cstat,ig)\n",
    "\n",
    "allSimilarities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row n here has the similarity between the nth company name in compustat and the IG company corresp to that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSimilarities[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find indices of companies in IG most similar to each company in CStat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestElementsCos = (-allSimilarities).argsort(axis=-1)[:, :n]\n",
    "singleLargestCos   = (-allSimilarities).argsort(axis=-1)[:, :1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the cosine similarity measures to the similarity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,companyMatches.shape[0]):\n",
    "    # print(list(np.array(hq.company)[largestElements[i]]))\n",
    "    companyMatches.at[i,'cosSimilarity']        = allSimilarities[i,:][singleLargestCos[i]]\n",
    "    companyMatches.at[i,'cosSimilarityCompany'] = igUnmatched.company[singleLargestCos[i]].iloc[0]\n",
    "    # companyMatches.at[i,'closestMatchCosine']   = np.array(igUnmatched.company)[largestElementsCos[i]]\n",
    "    # companyMatches.at[i,'cosineSim']            = np.array(allSimilarities[i,:][largestElementsCos[i]], dtype=object)\n",
    "    igUnmatched.company[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches.at[i,'costMatchIG_add']     = igUnmatched.address_line_1[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches.at[i,'cosMatchIG_city']     = igUnmatched.city[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches.at[i,'cosMatchIG_zipcode']  = igUnmatched.zipcode[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches.at[i,'cosMatchIG_state']    = igUnmatched.state[singleLargestCos[i]].iloc[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((companyMatches.levCompany == companyMatches.cosSimilarityCompany))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find the company matches: ABI - gvkey link.\n",
    "\n",
    "Start with ones where the names both match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bothMatch_cityZip = companyMatches[(companyMatches.levCompany == companyMatches.cosSimilarityCompany) & \\\n",
    "              ((companyMatches.cstatCity == companyMatches.closestMatchIG_city) | \\\n",
    "              (companyMatches.cstatZip == companyMatches.closestMatchIG_zipcode))]\n",
    "bothMatch_cityZip.shape\n",
    "\n",
    "bothMatch_cityZip.to_csv(\"../../data/companyData/bothMatch_cityZip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bothMatch_cityZip['igCompanies'] = bothMatch_cityZip.levCompany\n",
    "companiesToCheck                 = bothMatch_cityZip[['cstatCompanies','igCompanies']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatchesBoth = list(bothMatch_cityZip.cstatCompanies.unique())\n",
    "len(companyMatchesBoth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneMatch_cityZipOnly.cstatCity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneMatch_cityZipOnly = companyMatches[-(companyMatches.cstatCompanies.isin(companyMatchesBoth)) & \\\n",
    "              (((companyMatches.cstatCity == companyMatches.closestMatchIG_city) | \\\n",
    "              (companyMatches.cstatZip == companyMatches.closestMatchIG_zipcode)) | \\\n",
    "              ((companyMatches.cstatCity == companyMatches.cosMatchIG_city) | \\\n",
    "              (companyMatches.cstatZip == companyMatches.cosMatchIG_zipcode)))].reset_index(drop=True)\n",
    "\n",
    "oneMatch_cityZipOnly['igCompanies'] = ''\n",
    "\n",
    "for i in range(0,oneMatch_cityZipOnly.shape[0]):\n",
    "    if ((oneMatch_cityZipOnly.cstatCity[i] == oneMatch_cityZipOnly.closestMatchIG_city[i]) | \\\n",
    "              (oneMatch_cityZipOnly.cstatZip[i] == oneMatch_cityZipOnly.closestMatchIG_zipcode[i])):\n",
    "        oneMatch_cityZipOnly.loc[i,'igCompanies'] = companyMatches.levCompany[i]\n",
    "    else:\n",
    "        oneMatch_cityZipOnly.loc[i,'igCompanies'] = companyMatches.cosSimilarityCompany[i]\n",
    "\n",
    "# oneMatch_cityZipOnly.to_csv(\"../../data/companyData/oneMatch_cityZipOnly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companiesToCheck = companiesToCheck.append(oneMatch_cityZipOnly[['cstatCompanies','igCompanies']]).drop_duplicates()\n",
    "\n",
    "companiesToCheck.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqUnmatched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companiesToCheck.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqStillUnmatched = chqUnmatched[-chqUnmatched.company.isin(companiesToCheck.cstatCompanies)].reset_index(drop=True)\n",
    "igStillUnmatched  = igUnmatched[-igUnmatched.company.isin(companiesToCheck.igCompanies)].reset_index(drop=True)\n",
    "\n",
    "# companyMatches['cstatCompanies'] = chqUnmatched.company\n",
    "print(chqUnmatched.shape, chqStillUnmatched.shape, companiesToCheck.shape)\n",
    "print(igStillUnmatched.shape,igUnmatched.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 2\n",
    "Match remaining ones on first word of name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqUnmatched.company[0].split(' ')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the edit distance for the first words of the company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyArrayCStat = []\n",
    "\n",
    "start = time.time()\n",
    "for company in chqStillUnmatched.company:\n",
    "    thisCompany = []\n",
    "    for ig in igStillUnmatched.company:\n",
    "        thisCompany.append(levenshtein_distance(company.split(' ')[0],ig.split(' ')[0]))\n",
    "    \n",
    "    companyArrayCStat.append([thisCompany])\n",
    "\n",
    "allLD = np.concatenate(companyArrayCStat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqStillUnmatchedFirstCo = []\n",
    "igStillUnmatchedFirstCo = []\n",
    "\n",
    "for company in chqStillUnmatched.company:\n",
    "    chqStillUnmatchedFirstCo.append(company.split(' ')[0])\n",
    "    \n",
    "for company in igStillUnmatched.company:\n",
    "    igStillUnmatchedFirstCo.append(company.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqUnmatchedList = list(map(nlp, chqStillUnmatchedFirstCo))\n",
    "allCompaniesIG = list(map(nlp, igStillUnmatchedFirstCo))\n",
    "\n",
    "\n",
    "cstat = getMatrix(chqUnmatchedList)\n",
    "ig    = getMatrix(allCompaniesIG)\n",
    "\n",
    "allSimilarities = cosine_similarity(cstat,ig)\n",
    "\n",
    "allSimilarities.shape\n",
    "\n",
    "largestElementsCos = (-allSimilarities).argsort(axis=-1)[:, :n]\n",
    "singleLargestCos   = (-allSimilarities).argsort(axis=-1)[:, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igStillUnmatched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "singleLargestLV   = (allLD).argsort(axis=-1)[:, :1]\n",
    "largestElementsLV = (allLD).argsort(axis=-1)[:, :n]\n",
    "\n",
    "\n",
    "companyMatches2 = pd.DataFrame()\n",
    "companyMatches2['cstatCompanies'] = chqStillUnmatched.company\n",
    "companyMatches2['cstatadd1']      = chqStillUnmatched.cstatadd1\n",
    "companyMatches2['cstatCity']      = chqStillUnmatched.cstatCity\n",
    "companyMatches2['cstatState']     = chqStillUnmatched.cstatState\n",
    "companyMatches2['cstatZip']       = chqStillUnmatched.cstatZipcode\n",
    "\n",
    "for i in range(0,companyMatches2.shape[0]):\n",
    "    # print(list(np.array(hq.company)[largestElements[i]]))\n",
    "    companyMatches2.at[i,'misspelling']         = allLD[i,:][singleLargestLV[i]]\n",
    "    companyMatches2.at[i,'levCompany']          = igStillUnmatched.company[singleLargestLV[i]].iloc[0]\n",
    "    \n",
    "    companyMatches2.at[i,'closestMatchIG_add']      = igStillUnmatched.address_line_1[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches2.at[i,'closestMatchIG_city']     = igStillUnmatched.city[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches2.at[i,'closestMatchIG_zipcode']  = igStillUnmatched.zipcode[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches2.at[i,'closestMatchIG_state']    = igStillUnmatched.state[singleLargestLV[i]].iloc[0]\n",
    "\n",
    "    \n",
    "    companyMatches2.at[i,'cosSimilarity']        = allSimilarities[i,:][singleLargestCos[i]]\n",
    "    companyMatches2.at[i,'cosSimilarityCompany'] = igStillUnmatched.company[singleLargestCos[i]].iloc[0]\n",
    "   \n",
    "    companyMatches2.at[i,'costMatchIG_add']     = igStillUnmatched.address_line_1[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches2.at[i,'cosMatchIG_city']     = igStillUnmatched.city[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches2.at[i,'cosMatchIG_zipcode']  = igStillUnmatched.zipcode[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches2.at[i,'cosMatchIG_state']    = igStillUnmatched.state[singleLargestCos[i]].iloc[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find if city or zip match here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match2_cityZips = companyMatches2[(((companyMatches2.cstatCity == companyMatches2.closestMatchIG_city) | \\\n",
    "              (companyMatches2.cstatZip == companyMatches2.closestMatchIG_zipcode)) | \\\n",
    "              ((companyMatches2.cstatCity == companyMatches2.cosMatchIG_city) | \\\n",
    "              (companyMatches2.cstatZip == companyMatches2.cosMatchIG_zipcode)))].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match2_cityZips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match2_cityZips.to_csv(\"../../data/companyData/match2_cityZips.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 3\n",
    "Try the addresses here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqStillUnmatched['cstatadd1']     = chqStillUnmatched.cstatadd1.astype(str)\n",
    "igStillUnmatched['address_line_1'] = igStillUnmatched.address_line_1.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addressArrayCStat = []\n",
    "\n",
    "start = time.time()\n",
    "for address in chqStillUnmatched.cstatadd1:\n",
    "    thisAddress = []\n",
    "    for ig in igStillUnmatched.address_line_1:\n",
    "        thisAddress.append(levenshtein_distance(str(address),str(ig)))\n",
    "    \n",
    "    addressArrayCStat.append([thisAddress])\n",
    "\n",
    "allLD = np.concatenate(addressArrayCStat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqUnmatchedList = list(map(nlp, chqStillUnmatched['cstatadd1']))\n",
    "allCompaniesIG   = list(map(nlp, igStillUnmatched['address_line_1']))\n",
    "\n",
    "\n",
    "cstat = getMatrix(chqUnmatchedList)\n",
    "ig    = getMatrix(allCompaniesIG)\n",
    "\n",
    "allSimilarities = cosine_similarity(cstat,ig)\n",
    "\n",
    "allSimilarities.shape\n",
    "\n",
    "largestElementsCos = (-allSimilarities).argsort(axis=-1)[:, :n]\n",
    "singleLargestCos   = (-allSimilarities).argsort(axis=-1)[:, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "singleLargestLV   = (allLD).argsort(axis=-1)[:, :1]\n",
    "largestElementsLV = (allLD).argsort(axis=-1)[:, :n]\n",
    "\n",
    "\n",
    "companyMatches2 = pd.DataFrame()\n",
    "companyMatches2['cstatCompanies'] = chqStillUnmatched.company\n",
    "companyMatches2['cstatadd1']      = chqStillUnmatched.cstatadd1\n",
    "companyMatches2['cstatCity']      = chqStillUnmatched.cstatCity\n",
    "companyMatches2['cstatState']     = chqStillUnmatched.cstatState\n",
    "companyMatches2['cstatZip']       = chqStillUnmatched.cstatZipcode\n",
    "\n",
    "for i in range(0,companyMatches2.shape[0]):\n",
    "    # print(list(np.array(hq.company)[largestElements[i]]))\n",
    "    companyMatches2.at[i,'misspelling']         = allLD[i,:][singleLargestLV[i]]\n",
    "    companyMatches2.at[i,'levCompany']          = igStillUnmatched.company[singleLargestLV[i]].iloc[0]\n",
    "    \n",
    "    companyMatches2.at[i,'closestMatchIG_add']      = igStillUnmatched.address_line_1[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches2.at[i,'closestMatchIG_city']     = igStillUnmatched.city[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches2.at[i,'closestMatchIG_zipcode']  = igStillUnmatched.zipcode[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches2.at[i,'closestMatchIG_state']    = igStillUnmatched.state[singleLargestLV[i]].iloc[0]\n",
    "\n",
    "    \n",
    "    companyMatches2.at[i,'cosSimilarity']        = allSimilarities[i,:][singleLargestCos[i]]\n",
    "    companyMatches2.at[i,'cosSimilarityCompany'] = igStillUnmatched.company[singleLargestCos[i]].iloc[0]\n",
    "   \n",
    "    companyMatches2.at[i,'costMatchIG_add']     = igStillUnmatched.address_line_1[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches2.at[i,'cosMatchIG_city']     = igStillUnmatched.city[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches2.at[i,'cosMatchIG_zipcode']  = igStillUnmatched.zipcode[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches2.at[i,'cosMatchIG_state']    = igStillUnmatched.state[singleLargestCos[i]].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match2_cityZips = companyMatches2[(((companyMatches2.cstatCity == companyMatches2.closestMatchIG_city) | \\\n",
    "              (companyMatches2.cstatZip == companyMatches2.closestMatchIG_zipcode)) | \\\n",
    "              ((companyMatches2.cstatCity == companyMatches2.cosMatchIG_city) | \\\n",
    "              (companyMatches2.cstatZip == companyMatches2.cosMatchIG_zipcode)))].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match2_cityZips.to_csv(\"../../data/companyData/companyMatches2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
