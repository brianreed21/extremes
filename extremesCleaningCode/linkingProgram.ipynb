{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "\n",
    "import collections\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import rasterio\n",
    "\n",
    "import spacy\n",
    "  \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Full IG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../../data/companyData/infogroup2010s.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(file, assume_missing=True, \n",
    "                 dtype={'parent_number': 'object','parent_employee_size_code': 'object',\n",
    "                       'parent_sales_volume_code': 'object',\n",
    "                       'abi': 'object','zipcode': 'object'}, low_memory = False)\n",
    "df = df[df.business_status_code == 1.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abiRents = df[['abi','parent_number']].drop_duplicates().compute(num_workers = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abiRents.to_csv(\"../../data/abiRents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differentParents.to_csv(\"../../data/differentParents2010s.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq = df[['abi','ticker','company','archive_version_year','state','city',\n",
    "         'address_line_1','zipcode',\n",
    "         'latitude','longitude']].drop_duplicates().compute(num_workers = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly = hq[['abi','company']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly.company.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hq.shape,hqsOnly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the abi numbers seem to be duplicated; it looks like they might be primarily for different government agencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly.company.value_counts()[hqsOnly.company.value_counts() > 10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toDiscard = hqsOnly.company.value_counts()[hqsOnly.company.value_counts() > 1].index\n",
    "for company in toDiscard:\n",
    "    print(company)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toDiscard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly = hqsOnly[~hqsOnly.company.isin(toDiscard)]\n",
    "hq      = hq[~hq.company.isin(toDiscard)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a unique record of every company - hq here. Some of these may well be duplicate entries for a given company, for the cases in which we have a company that has multiple hq.\n",
    "\n",
    "Let's stash it so that we don't have to go through the above ^^ again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly.to_csv(\"../../data/ig2010s_uniqueHQs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq.to_csv(\"../../data/ig2010s_uniqueHQs_multLocations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Compustat Data\n",
    "\n",
    "First filter down to the companies for whom we have the supply chain information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_links = pd.read_csv(\"../../data/companyData/compustatSCLinked.csv\")\n",
    "\n",
    "c_links['year'] = c_links.srcdate.astype('str').str.slice(0,4).astype('int64')\n",
    "\n",
    "c_links = c_links[c_links.year > 2009]\n",
    "\n",
    "relevant_gvkeys = c_links.gvkey.append(c_links.cgvkey).drop_duplicates()\n",
    "\n",
    "print(c_links.head(),relevant_gvkeys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_links.gvkey.unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the company dataset and check.\n",
    "\n",
    "The legal name and the given name are slightly different, but basically the same modulo punctuation and case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_addresses = pd.read_csv(\"../../data/companyData/compustatAddresses.csv\", \n",
    "                dtype={'parent_number': 'object'})[['fyear',\n",
    "                'gvkey',\n",
    "                'conm',\n",
    "                'add1',\n",
    "                'city',\n",
    "                'state',\n",
    "                'idbflag',\n",
    "                'addzip',\n",
    "               'naics']].drop_duplicates().rename(columns = {'fyear': 'year'})\n",
    "c_addresses = c_addresses[(c_addresses.year > 2009) & (c_addresses.year < 2020) & \\\n",
    "                          c_addresses.gvkey.isin(relevant_gvkeys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_addresses.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can later, potentially, subset this to focus on firms in: ag, mining, construction, manufacturing, wholesale and retail, and transportation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''c_addresses = c_addresses[(c_addresses.naics.astype('str').str.slice(0,2).isin(['11','21','22','23','31','32',\n",
    "                                                         '33','42','44','45','48','49']))]\n",
    "'''\n",
    "chq = c_addresses[['gvkey','conm','add1','city','state','addzip','idbflag']].drop_duplicates()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're starting with the compustat north america dataset. Not all of the HQs are in North America, so we can filter some of the information down to match with Infogroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq.idbflag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canadian = ['ON', 'AB','QC', 'BC', 'NS', 'NF', 'SK', 'MB', 'NB']\n",
    "\n",
    "chq.state.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq = chq[~(chq.state.isin(canadian)) & ~chq.state.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq.addzip.str.len().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq['addzip'] = chq.addzip.astype('str').str.slice(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq[chq.idbflag == 'D'].addzip.str.len().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq[chq.idbflag == \"B\"].addzip.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chq.head(),chq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq.rename(columns = {'conm': 'company','addzip': 'cstatZipcode'},inplace = True)\n",
    "chq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq = chq[chq.gvkey.isin(relevant_gvkeys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq.to_csv(\"../../data/chq.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakpoint\n",
    "We can start from here and just clean everything from here.\n",
    "\n",
    "\n",
    "Headquarters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq     = pd.read_csv(\"../../data/chq.csv\",dtype={'cstatZipcode': 'object'}).drop(columns = {'Unnamed: 0'})\n",
    "\n",
    "chq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly     = pd.read_csv(\"../../data/ig2010s_uniqueHQs.csv\").drop(columns = {'Unnamed: 0'})\n",
    "hqsWithYear = pd.read_csv(\"../../data/ig2010s_uniqueHQs_multLocations.csv\",dtype={'zipcode': 'object'})[['abi','company',\n",
    "                                                                             'archive_version_year',\n",
    "                                                                             'state','city','zipcode','address_line_1']]\n",
    "\n",
    "print(hqsOnly.head())\n",
    "print(hqsWithYear.head())\n",
    "\n",
    "hqsWithYear = hqsWithYear[hqsWithYear.archive_version_year <= 2020]\n",
    "\n",
    "hqsWithYear['last_year'] = hqsWithYear.groupby(['abi'])['archive_version_year'].transform(max)\n",
    "\n",
    "print(hqsWithYear.shape)\n",
    "\n",
    "lastHQs = hqsWithYear[hqsWithYear.archive_version_year == hqsWithYear.last_year][['abi','company','state','city','zipcode','address_line_1']]\n",
    "\n",
    "print(lastHQs.shape)\n",
    "\n",
    "lastHQs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastHQs.to_csv(\"../../data/companyData/lastHQs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only two of these company names appear 2x, which is good. There are ~20,000 companies in this sample.\n",
    "\n",
    "Let's go through a little bit of a process here:\n",
    "- Find the exact matches.\n",
    "- Get a similarity measure between ; ideally something vectorized / something in matrix math.\n",
    "- Find the top 10 matches for the remaining ones.\n",
    "- Do some mix and match and see if there's any threshold at which matches become similar ``enough'' to say this is okay and good to go.\n",
    "\n",
    "\n",
    "We might be able to use the fact that all of the addresses should be the same after some given point, as the compustat addresses are only the most recent ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few different ways to match these up.\n",
    "\n",
    "First, let's find the exact matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a generic cleaning function that strips out all company names, any punctuation in the name, and makes everything lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = text.strip().\\\n",
    "    replace(\" CORP\",\"\").replace(\" CO\",\"\").replace(\" INC\",\"\").\\\n",
    "    replace(\" LTD\",\"\").replace(\" -CL A\",\"\").\\\n",
    "    replace(\" -LP\",\"\").replace(\" LP\",\"\").\\\n",
    "    replace(\"-OLD\",\"\").replace(\" LLC\",\"\").\\\n",
    "    replace(\", LLC.\",\"\").replace(\" L.L.C.\",\"\").replace(\" L.P.\",\"\").\\\n",
    "    replace(\" L.TD\",\"\").replace(\" L.L.C.\",\"\").replace(\" -CL B\",\"\").\\\n",
    "    replace(\" -CL B\",\"\").replace(\" -CL i\",\"\").replace(\" -CL\",\"\").\\\n",
    "    replace(\"-REDH\",\"\").replace(\" CP\",\"\").\\\n",
    "    replace(\"-ADR\",\"\").replace(\" PLC\",\"\").lower().replace(r'[^\\w\\s]+', '').\\\n",
    "    replace('-lp','').replace('-spn','').replace('hldg','').replace(' intl','').\\\n",
    "    replace('holdings','').replace('holding','').replace('prtnr','').replace('group','').\\\n",
    "    replace(\" med \", \" medical \").replace(\" tradng \", \" trading \").replace(\"gen \", \"general \").\\\n",
    "    replace(\" mtr \", \" motors \").replace(\" motor \", \" motors \").replace(\"-\", \" \").\\\n",
    "    replace(\"/\", \" \").replace(\"'\", \" \").replace(\"&\", \" \").replace(\" a g \", \" \").\\\n",
    "    replace(\" ag \", \" \").replace(\"  adr \", \" \").replace(\" adr \", \" \").replace(\"  cp \", \" \").\\\n",
    "    replace(\" cp \", \" \").replace(\" plc \", \" \").replace(\" intl \", \" \").replace(\" ent \", \" \").\\\n",
    "    replace(\" nv \", \" \").replace(\" n.v. \", \" \").replace(\" worldwide \", \" \").\\\n",
    "    replace(\" wldwide \", \" \").replace(\" banc\",\" bank\").replace(\"^banc\",\"bank\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq['company']               = list(map(cleanText, chq.company))\n",
    "lastHQs['company']           = list(map(cleanText, lastHQs.company))\n",
    "\n",
    "chq.rename(columns = {'city': 'cstatCity',\n",
    "                     'state': 'cstatState',\n",
    "                     'add1': 'cstatadd1'}, inplace = True)\n",
    "\n",
    "chq['cstatCity']  = chq.cstatCity.str.lower()\n",
    "chq['cstatState'] = chq.cstatState.str.lower()\n",
    "chq['cstatadd1']  = chq.cstatadd1.str.lower()\n",
    "\n",
    "lastHQs['city']            = lastHQs.city.str.lower()\n",
    "lastHQs['state']           = lastHQs.state.str.lower()\n",
    "lastHQs['address_line_1']  = lastHQs.address_line_1.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAICS names do not match up between compustat and infogroup so they're not helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastHQs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match on company name directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameMerge = chq.merge(lastHQs)\n",
    "nameMerge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameMerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameMerge.to_csv(\"../../data/companyData/nameMerge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(nameMerge.cstatState == nameMerge.state)/nameMerge.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(nameMerge.cstatZipcode.str.slice(0,5) == nameMerge.zipcode.str.slice(0,5))/nameMerge.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(nameMerge.cstatZipcode.str.slice(0,1) == nameMerge.zipcode.str.slice(0,1))/nameMerge.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameMerge[nameMerge.cstatCity != nameMerge.city][50:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now focus down onto the companies that have not been matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqUnmatched = chq[~chq.company.isin(nameMerge.company)].reset_index()\n",
    "chqUnmatched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igUnmatched  = lastHQs[~lastHQs.company.isin(nameMerge.company)].reset_index()\n",
    "igUnmatched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastHQs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two distance measures here. Look at top 5 matches and pull the distance measure and matches as well.\n",
    "\n",
    "### Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as levenshtein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find LD between the unmatched compustat companies and the unmatched IG ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyArrayCStat = []\n",
    "\n",
    "company = chqUnmatched.company[0]\n",
    "start = time.time()\n",
    "for company in chqUnmatched.company:\n",
    "    thisCompany = []\n",
    "    for ig in igUnmatched.company:\n",
    "        thisCompany.append(levenshtein_distance(company,ig))\n",
    "    \n",
    "    companyArrayCStat.append([thisCompany])\n",
    "\n",
    "allLD = np.concatenate(companyArrayCStat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igUnmatched.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pull the closest companies in IG to those in CStat. We'll first populate a dataframe with the name, address, city, state, and zip of each unmatched company in compustat, then we'll use the LD to find the same information for the closest company in IG.\n",
    "\n",
    "\n",
    "\n",
    "There's some legacy code in here that finds the top 5 closest companies; but it doesn't populate the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleLargestLV   = (allLD).argsort(axis=-1)[:, :1]\n",
    "\n",
    "# legacy\n",
    "# n = 5\n",
    "# largestElementsLV = (allLD).argsort(axis=-1)[:, :n]\n",
    "\n",
    "\n",
    "companyMatches = pd.DataFrame()\n",
    "companyMatches['cstatCompanies'] = chqUnmatched.company\n",
    "companyMatches['cstatadd1']      = chqUnmatched.cstatadd1\n",
    "companyMatches['cstatCity']      = chqUnmatched.cstatCity\n",
    "companyMatches['cstatState']     = chqUnmatched.cstatState\n",
    "companyMatches['cstatZip']       = chqUnmatched.cstatZipcode\n",
    "\n",
    "for i in range(0,companyMatches.shape[0]):\n",
    "    # print(list(np.array(hq.company)[largestElements[i]]))\n",
    "    companyMatches.at[i,'misspelling']         = allLD[i,:][singleLargestLV[i]]\n",
    "    companyMatches.at[i,'percMisspelled']      = companyMatches.misspelling[i]/len(companyMatches.cstatCompanies[i])\n",
    "    companyMatches.at[i,'levCompany']          = igUnmatched.company[singleLargestLV[i]].iloc[0]\n",
    "    \n",
    "    companyMatches.at[i,'closestMatchIG_add']      = igUnmatched.address_line_1[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches.at[i,'closestMatchIG_city']     = igUnmatched.city[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches.at[i,'closestMatchIG_zipcode']  = igUnmatched.zipcode[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches.at[i,'closestMatchIG_state']    = igUnmatched.state[singleLargestLV[i]].iloc[0]\n",
    "\n",
    "    \n",
    "    # companyMatches.at[i,'closestMatchIG']      = np.array(igUnmatched.company)[largestElementsLV[i]]\n",
    "    # companyMatches.at[i,'LevSim']              = np.array(allLD[i,:][largestElementsLV[i]], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the embeddings and the cosine similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatrix(companyEmbeddings):\n",
    "    companyArray = []\n",
    "    \n",
    "    for companies in companyEmbeddings:\n",
    "        companyArray.append([companies.vector])\n",
    "    \n",
    "    companyArray = np.concatenate(companyArray)\n",
    "    \n",
    "    return(companyArray)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqUnmatchedList = list(map(nlp, chqUnmatched.company))\n",
    "allCompaniesIG   = list(map(nlp, igUnmatched.company))\n",
    "\n",
    "\n",
    "cstat = getMatrix(chqUnmatchedList)\n",
    "ig    = getMatrix(allCompaniesIG)\n",
    "\n",
    "allSimilarities = cosine_similarity(cstat,ig)\n",
    "\n",
    "allSimilarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile =  '../../data/allCompaniesIG_embeddings.pkl'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pkl.dump(ig, pickle_file)\n",
    "    \n",
    "outfile =  '../../data/allCompaniesCStat_embeddings.pkl'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pkl.dump(cstat, pickle_file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row n here has the similarity between the nth company name in compustat and the IG company corresp to that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSimilarities[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find indices of companies in IG most similar to each company in CStat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleLargestCos   = (-allSimilarities).argsort(axis=-1)[:, :1]\n",
    "\n",
    "# legacy - largest n\n",
    "# largestElementsCos = (-allSimilarities).argsort(axis=-1)[:, :n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the cosine similarity measures to the similarity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,companyMatches.shape[0]):\n",
    "    # print(list(np.array(hq.company)[largestElements[i]]))\n",
    "    companyMatches.at[i,'cosSimilarity']        = allSimilarities[i,:][singleLargestCos[i]]\n",
    "    companyMatches.at[i,'cosSimilarityCompany'] = igUnmatched.company[singleLargestCos[i]].iloc[0]\n",
    "    # companyMatches.at[i,'closestMatchCosine']   = np.array(igUnmatched.company)[largestElementsCos[i]]\n",
    "    # companyMatches.at[i,'cosineSim']            = np.array(allSimilarities[i,:][largestElementsCos[i]], dtype=object)\n",
    "    \n",
    "    \n",
    "    companyMatches.at[i,'costMatchIG_add']     = igUnmatched.address_line_1[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches.at[i,'cosMatchIG_city']     = igUnmatched.city[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches.at[i,'cosMatchIG_zipcode']  = igUnmatched.zipcode[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches.at[i,'cosMatchIG_state']    = igUnmatched.state[singleLargestCos[i]].iloc[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((companyMatches.levCompany == companyMatches.cosSimilarityCompany))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Matching\n",
    "\n",
    "### Take 1: Match + Zip or City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find the company matches: ABI - gvkey link.\n",
    "\n",
    "Start with ones where the names both match.\n",
    "\n",
    "If the cities or zipcodes match on one of the closest companies (LD or cos), it seems like it is good to go.\n",
    "\n",
    "\n",
    "Do this in steps to start, at least. First find the companies where both match and either zip or city match. Then find companies where only one matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bothMatch_cityZip = companyMatches[(companyMatches.levCompany == companyMatches.cosSimilarityCompany) & \\\n",
    "              ((companyMatches.cstatCity == companyMatches.closestMatchIG_city) | \\\n",
    "              (companyMatches.cstatZip == companyMatches.closestMatchIG_zipcode))]\n",
    "bothMatch_cityZip.shape\n",
    "\n",
    "bothMatch_cityZip.to_csv(\"../../data/companyData/bothMatch_cityZip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bothMatch_cityZip['igCompanies'] = bothMatch_cityZip.levCompany\n",
    "companiesToCheck                 = bothMatch_cityZip[['cstatCompanies','igCompanies',\n",
    "                                    'cstatadd1','cstatCity','cstatZip',\n",
    "                                    'closestMatchIG_add','closestMatchIG_city','closestMatchIG_zipcode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatchesBoth = list(bothMatch_cityZip.cstatCompanies.unique())\n",
    "len(companyMatchesBoth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the single company match versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneMatch_cityZipOnly = companyMatches[-(companyMatches.cstatCompanies.isin(companyMatchesBoth)) & \\\n",
    "              (((companyMatches.cstatCity == companyMatches.closestMatchIG_city) | \\\n",
    "              (companyMatches.cstatZip == companyMatches.closestMatchIG_zipcode)) | \\\n",
    "              ((companyMatches.cstatCity == companyMatches.cosMatchIG_city) | \\\n",
    "              (companyMatches.cstatZip == companyMatches.cosMatchIG_zipcode)))].reset_index(drop=True)\n",
    "\n",
    "oneMatch_cityZipOnly['igCompanies'] = ''\n",
    "\n",
    "for i in range(0,oneMatch_cityZipOnly.shape[0]):\n",
    "    if ((oneMatch_cityZipOnly.cstatCity[i] == oneMatch_cityZipOnly.closestMatchIG_city[i]) | \\\n",
    "              (oneMatch_cityZipOnly.cstatZip[i] == oneMatch_cityZipOnly.closestMatchIG_zipcode[i])):\n",
    "        oneMatch_cityZipOnly.loc[i,'igCompanies'] = oneMatch_cityZipOnly.levCompany[i]\n",
    "    else:\n",
    "        oneMatch_cityZipOnly.loc[i,'igCompanies'] = oneMatch_cityZipOnly.cosSimilarityCompany[i]\n",
    "\n",
    "# oneMatch_cityZipOnly.to_csv(\"../../data/companyData/oneMatch_cityZipOnly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companiesToCheck.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneMatch_cityZipOnly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companiesToCheck = companiesToCheck.append(oneMatch_cityZipOnly[['cstatCompanies','igCompanies',\n",
    "                                    'cstatadd1','cstatCity','cstatZip',\n",
    "                                    'closestMatchIG_add','closestMatchIG_city','closestMatchIG_zipcode']]).\\\n",
    "                                    drop_duplicates()\n",
    "\n",
    "companiesToCheck.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companiesToCheck[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2x check that there are no duplicates here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = companiesToCheck.cstatCompanies.value_counts().index[companiesToCheck.cstatCompanies.value_counts() > 1]\n",
    "\n",
    "companiesToCheck[companiesToCheck.cstatCompanies.isin(duplicates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqStillUnmatched = chqUnmatched[-chqUnmatched.company.isin(companiesToCheck.cstatCompanies)].reset_index(drop=True)\n",
    "igStillUnmatched  = igUnmatched[-igUnmatched.company.isin(companiesToCheck.igCompanies)].reset_index(drop=True)\n",
    "\n",
    "# companyMatches['cstatCompanies'] = chqUnmatched.company\n",
    "print(chqUnmatched.shape, chqStillUnmatched.shape, companiesToCheck.shape)\n",
    "print(igStillUnmatched.shape,igUnmatched.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companiesToCheck.to_csv(\"../../data/companyData/companiesToCheck_cityZip.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 2\n",
    "Match remaining ones on first word of name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqUnmatched.company[0].split(' ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqUnmatched.company[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the edit distance for the first words of the company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyArrayCStat = []\n",
    "\n",
    "start = time.time()\n",
    "for company in chqStillUnmatched.company:\n",
    "    thisCompany = []\n",
    "    for ig in igStillUnmatched.company:\n",
    "        thisCompany.append(levenshtein_distance(company.split(' ')[0],ig.split(' ')[0]))\n",
    "    \n",
    "    companyArrayCStat.append([thisCompany])\n",
    "\n",
    "allLD = np.concatenate(companyArrayCStat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqStillUnmatchedFirstCo = []\n",
    "igStillUnmatchedFirstCo = []\n",
    "\n",
    "for company in chqStillUnmatched.company:\n",
    "    chqStillUnmatchedFirstCo.append(company.split(' ')[0])\n",
    "    \n",
    "for company in igStillUnmatched.company:\n",
    "    igStillUnmatchedFirstCo.append(company.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqUnmatchedList = list(map(nlp, chqStillUnmatchedFirstCo))\n",
    "allCompaniesIG = list(map(nlp, igStillUnmatchedFirstCo))\n",
    "\n",
    "\n",
    "cstat = getMatrix(chqUnmatchedList)\n",
    "ig    = getMatrix(allCompaniesIG)\n",
    "\n",
    "allSimilarities = cosine_similarity(cstat,ig)\n",
    "\n",
    "allSimilarities.shape\n",
    "\n",
    "# largestElementsCos = (-allSimilarities).argsort(axis=-1)[:, :n]\n",
    "singleLargestCos   = (-allSimilarities).argsort(axis=-1)[:, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igStillUnmatched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleLargestLV   = (allLD).argsort(axis=-1)[:, :1]\n",
    "# largestElementsLV = (allLD).argsort(axis=-1)[:, :n]\n",
    "\n",
    "\n",
    "companyMatches2 = pd.DataFrame()\n",
    "companyMatches2['cstatCompanies'] = chqStillUnmatched.company\n",
    "companyMatches2['cstatadd1']      = chqStillUnmatched.cstatadd1\n",
    "companyMatches2['cstatCity']      = chqStillUnmatched.cstatCity\n",
    "companyMatches2['cstatState']     = chqStillUnmatched.cstatState\n",
    "companyMatches2['cstatZip']       = chqStillUnmatched.cstatZipcode\n",
    "\n",
    "\n",
    "for i in range(0,companyMatches2.shape[0]):\n",
    "    # print(list(np.array(hq.company)[largestElements[i]]))\n",
    "    companyMatches2.at[i,'misspelling']         = allLD[i,:][singleLargestLV[i]]\n",
    "    companyMatches2.at[i,'levCompany']          = igStillUnmatched.company[singleLargestLV[i]].iloc[0]\n",
    "    \n",
    "    companyMatches2.at[i,'closestMatchIG_add']      = igStillUnmatched.address_line_1[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches2.at[i,'closestMatchIG_city']     = igStillUnmatched.city[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches2.at[i,'closestMatchIG_zipcode']  = igStillUnmatched.zipcode[singleLargestLV[i]].iloc[0]\n",
    "    companyMatches2.at[i,'closestMatchIG_state']    = igStillUnmatched.state[singleLargestLV[i]].iloc[0]\n",
    "\n",
    "    \n",
    "    companyMatches2.at[i,'cosSimilarity']        = allSimilarities[i,:][singleLargestCos[i]]\n",
    "    companyMatches2.at[i,'cosSimilarityCompany'] = igStillUnmatched.company[singleLargestCos[i]].iloc[0]\n",
    "   \n",
    "    companyMatches2.at[i,'costMatchIG_add']     = igStillUnmatched.address_line_1[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches2.at[i,'cosMatchIG_city']     = igStillUnmatched.city[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches2.at[i,'cosMatchIG_zipcode']  = igStillUnmatched.zipcode[singleLargestCos[i]].iloc[0]\n",
    "    companyMatches2.at[i,'cosMatchIG_state']    = igStillUnmatched.state[singleLargestCos[i]].iloc[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find if city or zip match here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match2_cityZips = companyMatches2[(((companyMatches2.cstatCity == companyMatches2.closestMatchIG_city) | \\\n",
    "              (companyMatches2.cstatZip == companyMatches2.closestMatchIG_zipcode)) | \\\n",
    "              ((companyMatches2.cstatCity == companyMatches2.cosMatchIG_city) | \\\n",
    "              (companyMatches2.cstatZip == companyMatches2.cosMatchIG_zipcode)))].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match2_cityZips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match2_cityZips['igCompanies'] = ''\n",
    "\n",
    "for i in range(0,match2_cityZips.shape[0]):\n",
    "    if ((match2_cityZips.cstatCity[i] == match2_cityZips.closestMatchIG_city[i]) | \\\n",
    "              (match2_cityZips.cstatZip[i] == match2_cityZips.closestMatchIG_zipcode[i])):\n",
    "        match2_cityZips.loc[i,'igCompanies'] = match2_cityZips.levCompany[i]\n",
    "    else:\n",
    "        match2_cityZips.loc[i,'igCompanies'] = match2_cityZips.cosSimilarityCompany[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match2_cityZips[['cstatCompanies','igCompanies','closestMatchIG_add','costMatchIG_add']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match2_cityZips[['cstatCompanies','igCompanies','closestMatchIG_add','costMatchIG_add']].to_csv(\"../../data/companyData/match2_cityZips.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 3\n",
    "Try the addresses here.\n",
    "\n",
    "Let's try something similar:\n",
    "- Find top 10 most similar addresses by cos sim\n",
    "- Find top 10 most similar addresses by LD\n",
    "- Find unique union of these two\n",
    "- Record LD and cos sim for each\n",
    "- Filter for totally dissimilar ones\n",
    "- ``Explode'' the dataset so we have cstat company, address\n",
    "- Find first word LD and cos sim\n",
    "- Find total LD and cos sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqStillUnmatched['cstatadd1']     = chqStillUnmatched.cstatadd1.astype(str)\n",
    "igStillUnmatched['address_line_1'] = igStillUnmatched.address_line_1.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addressArrayCStat = []\n",
    "\n",
    "start = time.time()\n",
    "for address in chqStillUnmatched.cstatadd1:\n",
    "    thisAddress = []\n",
    "    for ig in igStillUnmatched.address_line_1:\n",
    "        thisAddress.append(levenshtein_distance(str(address),str(ig)))\n",
    "    \n",
    "    addressArrayCStat.append([thisAddress])\n",
    "\n",
    "allLD = np.concatenate(addressArrayCStat)\n",
    "\n",
    "singleLargestLV   = (allLD).argsort(axis=-1)[:, :1]\n",
    "largestElementsLV = (allLD).argsort(axis=-1)[:, :n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chqUnmatchedList = list(map(nlp, chqStillUnmatched['cstatadd1']))\n",
    "allCompaniesIG   = list(map(nlp, igStillUnmatched['address_line_1']))\n",
    "\n",
    "\n",
    "cstat = getMatrix(chqUnmatchedList)\n",
    "ig    = getMatrix(allCompaniesIG)\n",
    "\n",
    "allSimilarities = cosine_similarity(cstat,ig)\n",
    "\n",
    "allSimilarities.shape\n",
    "\n",
    "singleLargestCos   = (-allSimilarities).argsort(axis=-1)[:, :1]\n",
    "largestElementsCos = (-allSimilarities).argsort(axis=-1)[:, :n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches3 = pd.DataFrame()\n",
    "companyMatches3['cstatCompanies'] = chqStillUnmatched.company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the set of the 10 closest addresses by LD and cos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the unique values in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(largestElementsLV[i]).union(set(largestElementsCos[i]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches3['closestAdds_indices'] = ''\n",
    "\n",
    "\n",
    "for i in range(0,companyMatches3.shape[0]):\n",
    "    \n",
    "    # find all the closest LV and cos addresses and put them in a \n",
    "    companyMatches3.at[i,'closestAdds_indices']  = set(largestElementsLV[i]).union(set(largestElementsCos[i]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And explode it so one index per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches3_indices = companyMatches3.explode('closestAdds_indices').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the companies, the cosine similarities, and the levenshtein distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches3_indices.closestAdds_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the embeddings for the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,companyMatches3_indices.shape[0]):\n",
    "    companyMatches3_indices.at[i,'igCompanies']  = igStillUnmatched.company[companyMatches3_indices.closestAdds_indices[i]] # .iloc[0]\n",
    "\n",
    "    companyMatches3_indices.at[i,'lv']           = levenshtein_distance(companyMatches3_indices.cstatCompanies[i],companyMatches3_indices.igCompanies[i])\n",
    "    companyMatches3_indices.at[i,'percMisspelled']       = companyMatches3_indices.lv[i]/len(companyMatches3_indices.cstatCompanies[i])\n",
    "\n",
    "    # companyMatches3.at[i,'add_percMisspelled'] = companyMatches2.add_misspelling[i]/len(companyMatches2['cstatadd1'][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the company embeddings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cMatches  = companyMatches3_indices[['cstatCompanies']].drop_duplicates()\n",
    "igMatches = companyMatches3_indices[['igCompanies']].drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVector(text):\n",
    "    embedding = nlp(text)\n",
    "    \n",
    "    return(embedding.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cStatEmbeddings = list(map(getVector, cMatches.cstatCompanies))\n",
    "igEmbeddings    = list(map(getVector, igMatches.igCompanies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igMatches['igEmbedding']       = igEmbeddings\n",
    "cMatches['cstatEmbedding']     = cStatEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches3_indices = companyMatches3_indices.merge(igMatches).merge(cMatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyMatches3_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through and get the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,companyMatches3_indices.shape[0]):\n",
    "    companyMatches3_indices.at[i,'cosSim']  = cosine_similarity([companyMatches3_indices.igEmbedding[i]],\n",
    "                                                                [companyMatches3_indices.cstatEmbedding[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = companyMatches3_indices[((companyMatches3_indices.percMisspelled < 0.4) | \\\n",
    "                        (companyMatches3_indices.cosSim > 0.6))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered[['cstatCompanies','igCompanies','percMisspelled','cosSim']].to_csv(\"../../data/companyData/companyMatches3_indices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all these things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset1 = pd.read_csv(\"../../data/companyData/companiesToCheck_cityZip.csv\")[['cstatCompanies','igCompanies','delete']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset2 = pd.read_csv(\"../../data/companyData/match2_cityZips.csv\")[['cstatCompanies','igCompanies','delete']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset3 = pd.read_csv(\"../../data/companyData/companyMatches3_indices.csv\")[['cstatCompanies','igCompanies','delete']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset1[['cstatCompanies','igCompanies','delete']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset2[['cstatCompanies','igCompanies','delete']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset3[['cstatCompanies','igCompanies','delete']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanMerge = pd.read_csv(\"../../data/companyData/nameMerge.csv\")[['company']].\\\n",
    "    rename(columns = {'company': 'cstatCompanies'})\n",
    "cleanMerge['igCompanies'] = cleanMerge['cstatCompanies']\n",
    "\n",
    "cleanMerge['delete'] = ''\n",
    "\n",
    "print(cleanMerge.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([dset1,dset2,dset3,cleanMerge])\n",
    "allFiltered = all[~(all.delete == 1.0)].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanMerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiltered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq     = pd.read_csv(\"../../data/chq.csv\",dtype={'cstatZipcode': 'object'}).drop(columns = {'Unnamed: 0'})\n",
    "\n",
    "chq['cstatCompanies'] = list(map(cleanText, chq.company))\n",
    "chqToMatch = chq[['cstatCompanies','gvkey']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = pd.read_csv(\"../../data/companyData/lastHQs.csv\")\n",
    "ig['igCompanies'] = list(map(cleanText, ig.company))\n",
    "\n",
    "igToMatch = ig[['igCompanies','abi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igToMatch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put these all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvKey_abiLinkingTable = allFiltered.merge(chqToMatch).merge(igToMatch).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvKey_abiLinkingTable.to_csv('../../data/companyData/linkingTable.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the SC Linking Table for 2010s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_links = pd.read_csv(\"../../data/companyData/compustatSCLinked.csv\")\n",
    "\n",
    "c_links['year'] = c_links.srcdate.astype('str').str.slice(0,4).astype('int64')\n",
    "\n",
    "c_links = c_links[c_links.year > 2009][['year','gvkey','cgvkey','salecs']].\\\n",
    "    rename(columns = {'cgvkey': 'customer_gvkey','gvkey': 'supplier_gvkey'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "c_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvKey_abiLinkingTable = pd.read_csv('../../data/companyData/linkingTable.csv')\n",
    "\n",
    "\n",
    "base_columns = gvKey_abiLinkingTable.columns \n",
    "customer_columns = \"customer_\" + base_columns\n",
    "supplier_columns = \"supplier_\" + base_columns\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "# merge in customer information\n",
    "gvKey_abiLinkingTable.columns = customer_columns\n",
    "\n",
    "print(c_links.shape)\n",
    "c_linksMerge1 = c_links.merge(gvKey_abiLinkingTable, on ='customer_gvkey')\n",
    "print(c_links.shape,c_linksMerge1.shape)\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "# and merge in supplier \n",
    "gvKey_abiLinkingTable.columns = supplier_columns\n",
    "\n",
    "print(c_links.shape)\n",
    "c_linksMerge2 = c_linksMerge1.merge(gvKey_abiLinkingTable, on ='supplier_gvkey')\n",
    "print(c_links.shape,c_linksMerge2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_linksMerge2.to_csv(\"../../data/companyData/clinks_IG_selected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvKey_abiLinkingTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably because: (1) companies are not in North America, or (2) companies are not in the physical goods industries we're interested in. We can verify this though: look at c_links where both the customer and supplier are in the dataset of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_linkTest = c_links[c_links.customer_gvkey.isin(chq.gvkey.unique()) & \\\n",
    "                     c_links.supplier_gvkey.isin(chq.gvkey.unique())]\n",
    "print(\"Percent of firms with a match: \", c_linksMerge2.shape[0]/c_linkTest.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's entirely possible that we have too small of a sample from the 2010s alone. Let's just try it though and see how it goes.\n",
    "\n",
    "First, make a sample with the companies on three years of either side of when it reports another customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scTableCustomers = c_linksMerge2.copy()[['year','customer_gvkey','customer_abi']]\n",
    "scTableSuppliers = c_linksMerge2.copy()[['year','supplier_gvkey','supplier_abi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scTableCustomers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeThreeEitherSide(df): \n",
    "    yrPlus1 = df.copy(); yrPlus1['year'] += 1\n",
    "    yrPlus2 = df.copy(); yrPlus2['year'] += 1\n",
    "    yrPlus3 = df.copy(); yrPlus3['year'] += 1\n",
    "    \n",
    "    yrMinus1 = df.copy(); yrMinus1['year'] -= 1\n",
    "    yrMinus2 = df.copy(); yrMinus2['year'] -= 1\n",
    "    yrMinus3 = df.copy(); yrMinus3['year'] -= 1\n",
    "    \n",
    "    all = pd.concat([yrPlus1,yrPlus2,yrPlus3,yrMinus1,yrMinus2,yrMinus3])\n",
    "    \n",
    "    return(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allCustomerData = makeThreeEitherSide(scTableCustomers)\n",
    "allCustomerData.columns = ['year','gvkey','abi']\n",
    "\n",
    "\n",
    "allSupplierData = makeThreeEitherSide(scTableSuppliers)\n",
    "allSupplierData.columns = ['year','gvkey','abi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSupplierData.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allAbi = allCustomerData.abi.append(allSupplierData.abi).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly = pd.read_csv(\"../../data/ig2010s_uniqueHQs.csv\").drop(columns = {'Unnamed: 0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq = pd.read_csv(\"../../data/ig2010s_uniqueHQs_multLocations.csv\").\\\n",
    "    drop(columns = {'Unnamed: 0'}).\\\n",
    "    rename(columns = {'archive_version_year': 'year'})\n",
    "\n",
    "hq['year'] = hq.year.astype('int64')\n",
    "\n",
    "hqRelevant = hq[hq.abi.isin(allAbi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqRelevant[hqRelevant.abi == 71340]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSupplierData = allSupplierData.merge(hqRelevant).drop_duplicates()\n",
    "allCustomerData = allCustomerData.merge(hqRelevant).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allCustomerData.to_csv(\"../../data/companyData/allCustomerData.csv\")\n",
    "allSupplierData.to_csv(\"../../data/companyData/allSupplierData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSupplierData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqsOnly = pd.read_csv(\"../../data/ig2010s_uniqueHQs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allAbi.isin(hqsOnly.abi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
