{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "\n",
    "import collections\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import rasterio\n",
    "\n",
    "import spacy\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "## Changes from year to year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = pd.read_csv(\"../../data/compustatChanges_all.csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "changes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherControls = pd.read_csv('../../data/companyData/otherControls.csv').\\\n",
    "    drop(columns = {'Unnamed: 0', 'fyearq'}).rename(columns = {'year_toMatchOn': 'year',\n",
    "                                                              'fqtr': 'qtr'})\n",
    "\n",
    "otherControls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherControls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(changes.shape)\n",
    "changes = changes.merge(otherControls)\n",
    "print(changes.shape)\n",
    "\n",
    "\n",
    "industries = changes[['gvkey','famafrench']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''changes.to_csv(\"../../data/companyData/compustatChanges_withControls.csv\")\n",
    "changes.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = pd.read_csv(\"../../data/companyData/compustatChanges_withControls.csv\").drop(columns = {'Unnamed: 0',\n",
    "                                                                                                 'Unnamed: 0.1',\n",
    "                                                                                                 'Unnamed: 0.1.1'})\n",
    "changes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put in the calendar quarters and fiscal quarter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarters = pd.read_csv(\"../../data/companyData/fiscalYears.csv\")\n",
    "quarters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(quarters.gvkey.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((quarters.fyr == 12) | \n",
    "   (quarters.fyr == 3) | \n",
    "   (quarters.fyr == 6) | \n",
    "   (quarters.fyr == 9))/quarters.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarters = quarters[(quarters.fyr == 12) | \n",
    "   (quarters.fyr == 3) | \n",
    "   (quarters.fyr == 6) | \n",
    "   (quarters.fyr == 9)][['gvkey','datadate','datacqtr','datafqtr','fyr']].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the quarter data into the change data, and make sure that the quarters that are used line up with the calendar quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesCal = changes[changes.gvkey.isin(quarters.gvkey.unique())]\n",
    "\n",
    "changesCal = changesCal.merge(quarters)\n",
    "\n",
    "print(changesCal.shape[0]/changes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesCal.loc[~(changesCal.datacqtr.isna()), 'year'] = changesCal.datacqtr.str.slice(0,4)\n",
    "changesCal.loc[~(changesCal.datacqtr.isna()), 'qtr']  = changesCal.datacqtr.str.slice(5,6)\n",
    "\n",
    "changesCal['DATE'] = pd.to_datetime(changesCal['datadate'])\n",
    "\n",
    "changesCal.loc[(changesCal.datacqtr.isna()), 'year'] = changesCal.DATE.dt.year\n",
    "changesCal.loc[(changesCal.datacqtr.isna()), 'qtr']  = changesCal.DATE.dt.quarter\n",
    "\n",
    "changesCal['year'] = changesCal.year.astype('int64')\n",
    "changesCal['qtr']  = changesCal.qtr.astype('int64')\n",
    "\n",
    "print(changesCal.shape,changesCal.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesCal.to_csv(\"../../data/companyData/compustatChanges_withControls.csv\")\n",
    "changesCal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesCal = pd.read_csv(\"../../data/companyData/compustatChanges_withControls.csv\")\n",
    "changesCal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compustat and ABI Linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvKey_abiLinkingTable = pd.read_csv('../../data/companyData/linkingTable.csv').drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "base_columns = gvKey_abiLinkingTable.columns \n",
    "customer_columns = \"customer_\" + base_columns\n",
    "supplier_columns = \"supplier_\" + base_columns\n",
    "\n",
    "\n",
    "\n",
    "hasMatch = gvKey_abiLinkingTable.gvkey.unique()\n",
    "\n",
    "gvKey_abiLinkingTable.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all change data together\n",
    "Get the linking table and merge the abi labels into the change df. \n",
    "\n",
    "Then, merge the location data into the change data and get as complete a record of companies as possible given the HQ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvKey_abiLinkingTable = pd.read_csv('../../data/companyData/linkingTable.csv').drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "changes = pd.read_csv(\"../../data/companyData/compustatChanges_withControls.csv\").drop(columns = ['Unnamed: 0'])\n",
    "print(changes.shape, changes.head())\n",
    "\n",
    "\n",
    "changesABI = changes.merge(gvKey_abiLinkingTable, on ='gvkey').drop(columns = {'state','city'})\n",
    "print(changesABI.shape, changesABI.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge in the hq information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canadian = ['ON', 'AB','QC', 'BC', 'NS', 'NF', 'SK', 'MB', 'NB']\n",
    "changes = changes[~(changes.state.isin(canadian)) & ~changes.state.isna()]\n",
    "\n",
    "changes['addzip'] = changes.addzip.astype('str').str.slice(0,5)\n",
    "\n",
    "changes.state.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes.drop(columns = {'Unnamed: 0.1'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq = pd.read_csv(\"../../data/ig_uniqueHQs_multLocations.csv\").\\\n",
    "    drop(columns = {'Unnamed: 0'}).\\\n",
    "    rename(columns = {'archive_version_year': 'year'})\n",
    "\n",
    "hq['year'] = hq.year.astype('int64')\n",
    "\n",
    "igChanges = changesABI.merge(hq)\n",
    "print(igChanges.shape, igChanges.head())\n",
    "\n",
    "\n",
    "hq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igChanges.to_csv(\"../../data/companyData/igData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have zip information in the following forms (from most to least examples):\n",
    "    - changes: all compustat companies, from the compustat address system\n",
    "    - igChanges: subset of compustat companies, from the ig merge\n",
    "    - subset of compustat companies that have SC information and survived the ig merge\n",
    "    \n",
    "We could potentially look at the subset of compustat companies for which we have SC information, usign the compustat address system as well.\n",
    "\n",
    "For now: follow similar trajectory as before but add in weather data for all cstat companies and all ig-merged companies.\n",
    "\n",
    "First: pull all zips that are mentioned in changes and igChanges and use this to get the weather data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = changes[(~changes.addzip.isna()) & (changes.addzip != 'nan')]\n",
    "relevantZips = changes.addzip.astype('int64').append(igChanges.zipcode).unique()\n",
    "\n",
    "changes.rename(columns = {'addzip': 'zipcode'}, inplace = True)\n",
    "changes.drop(columns = {'cik',\n",
    "     'datadate','costat', 'add1', 'add2', 'city', 'sic', 'state'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevantZips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevantZips = allCustomerData.zipcode.append(allSupplierData.zipcode).unique()\n",
    "outfile =  '../../data/companyData/relevantZips.pkl'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pkl.dump(relevantZips, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igChanges = pd.read_csv(\"../../data/companyData/igData.csv\").\\\n",
    "    drop(columns = {'Unnamed: 0'})\n",
    "igChanges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/stockReturns.pkl', 'rb') as f:\n",
    "    stocks = pkl.load(f)[['date','gvkey','RET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(stocks.gvkey.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = stocks[stocks.date.dt.year > 2008]\n",
    "\n",
    "stocks['qtr']  = stocks.date.dt.quarter\n",
    "stocks['year'] = stocks.date.dt.year\n",
    "\n",
    "stocks = stocks[~stocks.gvkey.isna()]\n",
    "stocks['gvkey'] = stocks['gvkey'].astype(int)\n",
    "stocks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igChanges.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyControls = igChanges[['gvkey','year','qtr','famafrench','ageTercile','sizeTercile','profitTercile','zipcode']]\n",
    "companyControls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stocks.dtypes, companyControls.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocksWithControls = stocks.merge(companyControls)\n",
    "print(stocksWithControls.shape,stocks.shape,companyControls.shape)\n",
    "stocksWithControls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del stocks\n",
    "del companyControls\n",
    "del igChanges\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annualWeather = pd.read_csv(\"../../data/companyData/stockWeather_annual.csv\").\\\n",
    "    drop(columns = {'Unnamed: 0'})\n",
    "\n",
    "annualWeather = annualWeather[~annualWeather.temp_annualLast5.isna()].reset_index(drop = True)\n",
    "\n",
    "annualWeather['date'] = pd.to_datetime(annualWeather['date'],\n",
    "                                   format = \"%Y%m%d\")\n",
    "\n",
    "annualWeather.rename(columns = {'ZIP': 'zipcode'}, inplace = True)\n",
    "print(annualWeather.dtypes)\n",
    "annualWeather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allWeather = pd.read_csv(\"../../data/companyData/stockWeather_zipQuarterQuants.csv\").\\\n",
    "    drop(columns = {'Unnamed: 0'})\n",
    "\n",
    "allWeather = allWeather[~allWeather.temp_zipQuarterLast5.isna()].reset_index(drop = True)\n",
    "\n",
    "allWeather['date'] = pd.to_datetime(allWeather['date'],\n",
    "                                   format = \"%Y-%m-%d\")\n",
    "\n",
    "allWeather.rename(columns = {'ZIP': 'zipcode'}, inplace = True)\n",
    "print(allWeather.dtypes)\n",
    "allWeather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocksWithControlsWeather = stocksWithControls.merge(allWeather).merge(annualWeather)\n",
    "print(stocksWithControlsWeather.shape,allWeather.shape)\n",
    "\n",
    "stocksWithControlsWeather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocksWithControlsWeather.to_csv(\"../../data/2+32+100+600sfgcompanyData/stocksWithControlsWeather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(stocksWithControlsWeather.RET.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocksWithControlsWeather = pd.read_csv(\"../../data/companyData/stocksWithControlsWeather.csv\").drop(columns = {'Unnamed: 0'})\n",
    "stocksWithControlsWeather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(stocksWithControlsWeather.gvkey.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allWeather = pd.read_csv(\"../../../../../../../Volumes/backup2/dissData/prism/allWeatherBins_2010.2019.csv\").\\\n",
    "allWeather = pd.read_csv(\"../../data/companyData/revised_allWeatherBins_2009to2019_allZips.csv\").\\\n",
    "    drop(columns = {\"Unnamed: 0\"})\n",
    "\n",
    "allWeather['yearQtr'] = allWeather.year + (allWeather.qtr - 1)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWeather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = allWeather.pop(\"yearQtr\")\n",
    "allWeather.insert(0, col.name, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWeather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag1 = allWeather.copy()\n",
    "lag1['yearQtr'] += 0.25\n",
    "for colname in lag1.columns[4:]:\n",
    "    lag1.rename(columns = {colname: 'lag1_' + colname}, inplace = True)\n",
    "lag1.drop(columns = {'year','qtr'},inplace = True)\n",
    "\n",
    "    \n",
    "lag2 = allWeather.copy()\n",
    "lag2['yearQtr'] += 0.5\n",
    "for colname in lag2.columns[4:]:\n",
    "    lag2.rename(columns = {colname: 'lag2_' + colname}, inplace = True)\n",
    "lag2.drop(columns = {'year','qtr'},inplace = True)\n",
    "\n",
    "\n",
    "lag3 = allWeather.copy()\n",
    "lag3['yearQtr'] += 0.75\n",
    "for colname in lag3.columns[4:]:\n",
    "    lag3.rename(columns = {colname: 'lag3_' + colname}, inplace = True)\n",
    "lag3.drop(columns = {'year','qtr'},inplace = True)\n",
    "\n",
    "\n",
    "lag4 = allWeather.copy()\n",
    "lag4['yearQtr'] += 1\n",
    "for colname in lag4.columns[4:]:\n",
    "    lag4.rename(columns = {colname: 'lag4_' + colname}, inplace = True)\n",
    "lag4.drop(columns = {'year','qtr'},inplace = True)\n",
    "\n",
    "\n",
    "print(allWeather.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWeather_withLags = allWeather.merge(lag1).merge(lag2).merge(lag3).merge(lag4)\n",
    "\n",
    "allWeather_withLags.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWeather_withLags.to_csv(\"../../data/companyData/allWeather_withLags_allZips.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for the industry-specific weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allWeather = pd.read_csv(\"../../../../../../../Volumes/backup2/dissData/prism/allWeatherBins_2010.2019.csv\").\\\n",
    "allWeather_byInd = pd.read_csv(\"../../data/companyData/revised_allWeatherBins_2009to2019_byInd.csv\").\\\n",
    "    drop(columns = {\"Unnamed: 0\"})\n",
    "'''[['famafrench','zipcode','yearQuarter', \n",
    "                                    'temp_ffquant_0.95','temp_indQuarterquant_0.95',\n",
    "                                   'temp5Days_ffquant_0.95', 'temp5Days_indQuarterquant_0.95',\n",
    "                                   'precip_ffquant_0.95', 'precip_indQuarterquant_0.95',\n",
    "                                   'precip5Days_ffquant_0.95', 'precip5Days_indQuarterquant_0.95']]\n",
    "'''\n",
    "allWeather_byInd['year'] = allWeather_byInd.yearQuarter.str.slice(0,4).astype('int64')\n",
    "allWeather_byInd['qtr']  = allWeather_byInd.yearQuarter.str.slice(5,6).astype('int64')\n",
    "allWeather_byInd['yearQtr'] = allWeather_byInd.year + (allWeather_byInd.qtr - 1)/4\n",
    "\n",
    "allWeather_byInd = allWeather_byInd.astype({'year':       'category',\n",
    "                         'qtr':        'category',\n",
    "                         'zipcode':    'category',\n",
    "                         'famafrench': 'category'})\n",
    "\n",
    "changes['zipcode'] = changes['zipcode'].astype({'zipcode': 'int64'})\n",
    "\n",
    "changes = changes.astype({'year':       'category',\n",
    "                          'qtr':        'category',\n",
    "                          'zipcode':    'category',\n",
    "                          'famafrench': 'category'})\n",
    "\n",
    "col = allWeather_byInd.pop(\"year\")\n",
    "allWeather_byInd.insert(0, col.name, col)\n",
    "\n",
    "col = allWeather_byInd.pop(\"qtr\")\n",
    "allWeather_byInd.insert(0, col.name, col)\n",
    "\n",
    "\n",
    "col = allWeather_byInd.pop(\"yearQtr\")\n",
    "allWeather_byInd.insert(0, col.name, col)\n",
    "\n",
    "allWeather_byInd.drop(columns = {'yearQuarter'}, inplace = True)\n",
    "\n",
    "print(allWeather_byInd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag1 = allWeather_byInd.copy()\n",
    "lag1['yearQtr'] += 0.25\n",
    "for colname in lag1.columns[5:]:\n",
    "    lag1.rename(columns = {colname: 'lag1_' + colname}, inplace = True)\n",
    "lag1.drop(columns = {'year','qtr'},inplace = True)\n",
    "lag1 = lag1.astype({'yearQtr':       'category'})\n",
    "\n",
    "    \n",
    "lag2 = allWeather_byInd.copy()\n",
    "lag2['yearQtr'] += 0.5\n",
    "for colname in lag2.columns[5:]:\n",
    "    lag2.rename(columns = {colname: 'lag2_' + colname}, inplace = True)\n",
    "lag2.drop(columns = {'year','qtr'},inplace = True)\n",
    "lag2 = lag2.astype({'yearQtr':       'category'})\n",
    "\n",
    "\n",
    "lag3 = allWeather_byInd.copy()\n",
    "lag3['yearQtr'] += 0.75\n",
    "for colname in lag3.columns[5:]:\n",
    "    lag3.rename(columns = {colname: 'lag3_' + colname}, inplace = True)\n",
    "lag3.drop(columns = {'year','qtr'},inplace = True)\n",
    "lag3 = lag3.astype({'yearQtr':       'category'})\n",
    "\n",
    "\n",
    "lag4 = allWeather_byInd.copy()\n",
    "lag4['yearQtr'] += 1\n",
    "for colname in lag4.columns[5:]:\n",
    "    lag4.rename(columns = {colname: 'lag4_' + colname}, inplace = True)\n",
    "lag4.drop(columns = {'year','qtr'},inplace = True)\n",
    "lag4 = lag4.astype({'yearQtr':       'category'})\n",
    "\n",
    "\n",
    "allWeather_byInd = allWeather_byInd.astype({'yearQtr':       'category'})\n",
    "\n",
    "\n",
    "print(allWeather_byInd.shape)\n",
    "\n",
    "\n",
    "allWeather_byInd.head()\n",
    "\n",
    "\n",
    "'''allWeather_byInd_withLags = allWeather_byInd.merge(lag1).merge(lag2).merge(lag3).merge(lag4)\n",
    "\n",
    "allWeather_byInd_withLags.year.value_counts()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWeather_byInd_withLags = allWeather_byInd.merge(lag1).merge(lag2).merge(lag3).merge(lag4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWeather_byInd_withLags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWeather_byInd_withLags.to_csv(\"../../data/companyData/allWeather_byInd_withLags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del allWeather_byInd\n",
    "del lag1\n",
    "del lag2\n",
    "del lag3\n",
    "del lag4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locations\n",
    "Create a separate definition of weather based not on HQ but on employee-weighted establishment footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = pd.read_csv('../../data/companyData/fractionEmployees_byEstablishment.csv').\\\n",
    "    drop(columns = {\"Unnamed: 0\", 'latitude','longitude'}).rename(columns = {'archive_version_year': 'year',\n",
    "                                                    'parent_number': 'abi'})\n",
    "\n",
    "fractions['year']    = fractions.year.astype('int64')\n",
    "fractions['zipcode'] = fractions.zipcode.astype('int64')\n",
    "fractions.head()\n",
    "\n",
    "gvKey_abiLinkingTable = pd.read_csv('../../data/companyData/linkingTable.csv').drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "print(gvKey_abiLinkingTable.abi)\n",
    "\n",
    "gvKey_abiLinkingTable.head()\n",
    "\n",
    "fractions = fractions[['year','abi','zipcode','locationFracOfEmployees']].merge(gvKey_abiLinkingTable[['abi','gvkey']])\n",
    "\n",
    "fractions = fractions.astype({'year':       'category',\n",
    "                           'zipcode':    'category'})\n",
    "\n",
    "fractions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractionsWithWeather = fractions.merge(allWeather_withLags) \n",
    "fractionsWithWeather.drop(columns = {'abi','zipcode'}, inplace = True)\n",
    "\n",
    "print(fractionsWithWeather.shape)\n",
    "fractionsWithWeather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractionsWithWeather[fractionsWithWeather.gvkey == 1004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del allWeather_withLags\n",
    "del fractions\n",
    "del gvKey_abiLinkingTable\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in fractionsWithWeather.columns[4:]:\n",
    "    fractionsWithWeather[col] = fractionsWithWeather[col] * fractionsWithWeather.locationFracOfEmployees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = fractionsWithWeather.groupby(['gvkey','year','qtr']).sum().reset_index()\n",
    "g.drop(columns = {'locationFracOfEmployees'}, inplace = True)\n",
    "\n",
    "for colname in g.columns[3:]:\n",
    "    g.rename(columns = {colname: 'empWt_' + colname}, inplace = True)\n",
    "\n",
    "g.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.to_csv(\"../../data/companyData/weatherByEstablishment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "establishmentZips = fractions.zipcode.unique()\n",
    "len(establishmentZips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the original weather with lags dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWeather_withLags = pd.read_csv(\"../../data/companyData/allWeather_withLags.csv\").\\\n",
    "    drop(columns = {\"Unnamed: 0\", 'yearQtr'})\n",
    "\n",
    "averages = pd.read_csv(\"../../data/companyData/quarterlyStatsByZip.csv\").\\\n",
    "    drop(columns = {\"Unnamed: 0\"}).rename(columns = {'ZIP': 'zipcode'})\n",
    "\n",
    "\n",
    "averages['qtr'] = averages.quarter.str.slice(1,2).astype('float')\n",
    "averages.drop(columns = {'quarter'}, inplace = True) \n",
    "\n",
    "print(len(averages.zipcode.unique()))\n",
    "\n",
    "averages.head()\n",
    "\n",
    "allWeather_withLags       = allWeather_withLags.astype({'year':       'category',\n",
    "                           'qtr':        'category',\n",
    "                           'zipcode':    'category'})\n",
    "averages                  = averages.astype({'qtr':        'category',\n",
    "                           'zipcode':    'category'})\n",
    "\n",
    "allWeather_byInd_withLags = pd.read_csv(\"../../data/companyData/allWeather_byInd_withLags.csv\").\\\n",
    "    drop(columns = {\"Unnamed: 0\", 'yearQtr'})\n",
    "allWeather_byInd_withLags = allWeather_byInd_withLags.astype({'year':       'category',\n",
    "                           'qtr':        'category',\n",
    "                           'zipcode':    'category'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create direct effects database. Merge weather to full cstat database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWeather_withLags.zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes.zipcode = changes.zipcode.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes['zipcode']  = changes['zipcode'].astype('int64')\n",
    "changesWithWeather = changes.merge(allWeather_withLags).merge(allWeather_byInd_withLags).merge(averages).merge(g)\n",
    "print(changes.shape,changesWithWeather.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesWithWeather.to_csv(\"../../data/companyData/cstatWithWeather.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge weather to the ig-cstat database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igChangesWithWeather = igChanges.merge(allWeather_withLags).merge(allWeather_byInd_withLags).merge(averages).merge(g)\n",
    "igChangesWithWeather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igChangesWithWeather.to_csv(\"../../data/companyData/igWithWeather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igChangesWithWeather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igChangesWithWeather['temp_zipquant_0.95'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igChangesWithWeather['temp5Days_ffquant_0.95'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indirect\n",
    "Introduce the SC Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does a little bit of a test on the reporting requirements. \n",
    "# number \n",
    "\n",
    "'''c_linksTest = pd.read_csv(\"../../data/companyData/compustatSCLinked.csv\")[['srcdate','gvkey','cgvkey']]\n",
    "c_linksTest['year'] = c_linksTest.srcdate.astype('str').str.slice(0,4).astype('int64')\n",
    "\n",
    "bs = c_linksTest[c_linksTest.year < 2014]\n",
    "print(\"Customers per supplier, 1978-2013 Pd: \", len(bs.cgvkey.unique())/len(bs.gvkey.unique()))\n",
    "\n",
    "bs2 = c_linksTest[c_linksTest.year > 2010]\n",
    "print(\"Customers per supplier, Recent Pd: \", len(bs2.cgvkey.unique())/len(bs2.gvkey.unique()))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_links = pd.read_csv(\"../../data/companyData/compustatSCLinked.csv\")\n",
    "\n",
    "c_links['year'] = c_links.srcdate.astype('str').str.slice(0,4).astype('int64')\n",
    "\n",
    "c_links = c_links[c_links.year > 1999][['year','gvkey','cgvkey','salecs']].\\\n",
    "    rename(columns = {'cgvkey': 'customer_gvkey','gvkey': 'supplier_gvkey'})\n",
    "\n",
    "\n",
    "print(c_links.shape)\n",
    "\n",
    "c_links.head()\n",
    "\n",
    "industries.columns = ['customer_gvkey','customer_famafrench']\n",
    "\n",
    "c_links = c_links.merge(industries)\n",
    "\n",
    "industries.columns = ['supplier_gvkey','supplier_famafrench']\n",
    "\n",
    "c_links = c_links.merge(industries)\n",
    "print(c_links.head(), c_links.shape)\n",
    "\n",
    "\n",
    "c_links.to_csv(\"../../data/companyData/c_links.csv\")\n",
    "\n",
    "\n",
    "sum(c_links.supplier_gvkey.isin(hasMatch) | c_links.customer_gvkey.isin(hasMatch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see if it's common to have one in and one out of the industries of interest. \n",
    "\n",
    "For now, let's keep all the different industry types.\n",
    "\n",
    "We can always filter later if we need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# merge in customer information\n",
    "gvKey_abiLinkingTable.columns = customer_columns\n",
    "\n",
    "print(c_links.shape)\n",
    "c_linksMerge1 = c_links.merge(gvKey_abiLinkingTable, on ='customer_gvkey')\n",
    "print(c_links.shape,c_linksMerge1.shape)\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "# and merge in supplier \n",
    "gvKey_abiLinkingTable.columns = supplier_columns\n",
    "\n",
    "print(c_links.shape)\n",
    "c_linksMerge2 = c_linksMerge1.merge(gvKey_abiLinkingTable, on ='supplier_gvkey')\n",
    "print(c_links.shape,c_linksMerge2.shape)\n",
    "\n",
    "c_linksMerge2.to_csv(\"../../data/companyData/clinks_IG_selected.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably because: (1) companies are not in North America, or (2) companies are not in the physical goods industries we're interested in. We can verify this though: look at c_links where both the customer and supplier are in the dataset of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chq     = pd.read_csv(\"../../data/chq.csv\",dtype={'cstatZipcode': 'object'}).drop(columns = {'Unnamed: 0'})\n",
    "\n",
    "c_linkTest = c_links[c_links.customer_gvkey.isin(chq.gvkey.unique()) & \\\n",
    "                     c_links.supplier_gvkey.isin(chq.gvkey.unique())]\n",
    "\n",
    "print(\"Percent of firms with a match: \", c_linksMerge2.shape[0]/c_linkTest.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's entirely possible that we have too small of a sample from the 2010s alone. Let's just try it though and see how it goes.\n",
    "\n",
    "First, make a sample with the companies on three years of either side of when it reports another customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeOneEitherSide(df): \n",
    "    yrPlus1 = df.copy(); yrPlus1['year'] += 1\n",
    "    # yrPlus2 = df.copy(); yrPlus2['year'] += 1\n",
    "    # yrPlus3 = df.copy(); yrPlus3['year'] += 1\n",
    "    \n",
    "    yrMinus1 = df.copy(); yrMinus1['year'] -= 1\n",
    "    # yrMinus2 = df.copy(); yrMinus2['year'] -= 1\n",
    "    # yrMinus3 = df.copy(); yrMinus3['year'] -= 1\n",
    "    \n",
    "    all = pd.concat([yrPlus1,yrMinus1]) # pd.concat([yrPlus1,yrPlus2,yrPlus3,yrMinus1,yrMinus2,yrMinus3])\n",
    "    \n",
    "    return(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scTableCustomers = c_linksMerge2.copy()[['year','customer_gvkey','customer_abi']].drop_duplicates()\n",
    "scTableSuppliers = c_linksMerge2.copy()[['year','supplier_gvkey','supplier_abi']].drop_duplicates()\n",
    "\n",
    "allCustomerData = makeOneEitherSide(scTableCustomers)\n",
    "allCustomerData.columns = ['year','gvkey','abi']\n",
    "\n",
    "\n",
    "allSupplierData = makeOneEitherSide(scTableSuppliers)\n",
    "allSupplierData.columns = ['year','gvkey','abi']\n",
    "\n",
    "allAbi = allCustomerData.abi.append(allSupplierData.abi).drop_duplicates()\n",
    "\n",
    "hqsOnly = pd.read_csv(\"../../data/ig_uniqueHQs.csv\").drop(columns = {'Unnamed: 0'})\n",
    "\n",
    "hq = pd.read_csv(\"../../data/ig_uniqueHQs_multLocations.csv\").\\\n",
    "    drop(columns = {'Unnamed: 0'}).\\\n",
    "    rename(columns = {'archive_version_year': 'year'})\n",
    "\n",
    "hq['year'] = hq.year.astype('int64')\n",
    "\n",
    "hqRelevant = hq[hq.abi.isin(allAbi)]\n",
    "\n",
    "\n",
    "allSupplierData = allSupplierData.merge(hqRelevant).drop_duplicates()\n",
    "allCustomerData = allCustomerData.merge(hqRelevant).drop_duplicates()\n",
    "\n",
    "print(allSupplierData.head())\n",
    "\n",
    "allCustomerData.to_csv(\"../../data/companyData/allCustomerData.csv\")\n",
    "allSupplierData.to_csv(\"../../data/companyData/allSupplierData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Customer and Supplier pairings and merge with change data\n",
    "### Can pick up here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSupplierData = pd.read_csv(\"../../data/companyData/allSupplierData.csv\").drop(columns = ['Unnamed: 0'])\n",
    "allCustomerData = pd.read_csv(\"../../data/companyData/allCustomerData.csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "changes = pd.read_csv(\"../../data/companyData/compustatChanges_withControls.csv\").drop(columns = ['Unnamed: 0'])\n",
    "changes.head()\n",
    "suppliers = changes.merge(allSupplierData[['year','gvkey','zipcode','employeesAtLocation']])\n",
    "print(suppliers.shape)\n",
    "\n",
    "customers = changes.merge(allCustomerData[['year','gvkey','zipcode','employeesAtLocation']])\n",
    "print(customers.head())\n",
    "\n",
    "print(allCustomerData.shape,allSupplierData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get first-hop SC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_links = pd.read_csv(\"../../data/companyData/clinks_IG_selected.csv\").drop(columns = {'Unnamed: 0'})\n",
    "c_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_links['suppliers'] = 1\n",
    "custExp = c_links[['year', 'customer_gvkey', 'salecs','suppliers']].groupby(['year','customer_gvkey']).sum().\\\n",
    "    reset_index().rename(columns = {'salecs': 'totalExp'})\n",
    "\n",
    "custExp.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of firms with no exp information and multiple suppliers: \", \\\n",
    "          sum(custExp[custExp.totalExp == 0].suppliers > 1))\n",
    "print(\"Number of firms with no exp information and >5 suppliers: \", \\\n",
    "          sum(custExp[custExp.totalExp == 0].suppliers > 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these firms have expenditure information. We can look at:\n",
    "    - Expenditure-weighted (just do equal shares if no exp information)\n",
    "    - Largest supplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB = c_links[['year','customer_gvkey','supplier_gvkey','salecs']].\\\n",
    "    merge(custExp).rename(columns = {'customer_gvkey': 'gvkey'}).drop_duplicates()\n",
    "print(customerDB.shape)\n",
    "\n",
    "customerDB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customersWithWeather = customers.merge(allWeather_withLags).merge(averages).merge(allWeather_byInd_withLags)\n",
    "customersWithWeather.shape\n",
    "\n",
    "suppliersWithWeather = suppliers.merge(allWeather_withLags).merge(averages).merge(allWeather_byInd_withLags)\n",
    "suppliersWithWeather.shape\n",
    "\n",
    "suppliersWithWeather.to_csv(\"../../data/companyData/suppliersWithWeather.csv\")\n",
    "customersWithWeather.to_csv(\"../../data/companyData/customersWithWeather.csv\")\n",
    "\n",
    "'''suppliersWithWeather = pd.read_csv(\"../../data/companyData/suppliersWithWeather.csv\").drop(columns = {'Unnamed: 0'})\n",
    "customersWithWeather = pd.read_csv(\"../../data/companyData/customersWithWeather.csv\").drop(columns = {'Unnamed: 0'})'''\n",
    "\n",
    "frames = [customersWithWeather, suppliersWithWeather]\n",
    "\n",
    "allCompanies = pd.concat(frames).drop_duplicates()\n",
    "\n",
    "print(allCompanies.shape)\n",
    "\n",
    "allCompanies.to_csv(\"../../data/companyData/allCompaniesWithWeather.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biggest Supplier\n",
    "Focus on weather of biggest supplier.\n",
    "\n",
    "First find the max by supplier. Add back in any rows with only 1 supplier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "idx = customerDB.groupby(['year','gvkey']).salecs.\\\n",
    "    transform(max) == customerDB.salecs\n",
    "largestSuppliers = customerDB[idx].reset_index(drop = True)\n",
    "print(largestSuppliers.shape)\n",
    "\n",
    "# find companies who only have one other supplier\n",
    "singleSuppliers = customerDB[customerDB.suppliers == 1].reset_index(drop = True)\n",
    "print(singleSuppliers.shape)\n",
    "\n",
    "# find largest suppliers of different companies\n",
    "largestSuppliers = largestSuppliers.append(singleSuppliers).drop_duplicates()\n",
    "print(largestSuppliers.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB[['year','gvkey']][customerDB.year > 2009].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(largestSuppliers.gvkey.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in suppliersWithWeather.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantVars = [x for x in suppliersWithWeather.columns if (('year' in x) | \n",
    "                                                 ('qtr' in x) |\n",
    "                                                 ('gvkey' in x) |\n",
    "                                                 ('_' in x)) & \n",
    "                                                ~('roa_lagged' in x) & \n",
    "                                                ~('yearQtr' in x)]\n",
    "\n",
    "suppliers_toMerge = suppliersWithWeather[relevantVars]\n",
    "\n",
    "\n",
    "for colname in suppliers_toMerge.columns[3:]:\n",
    "    suppliers_toMerge.rename(columns = {colname: 'supplier_' + colname}, inplace = True)\n",
    "\n",
    "    \n",
    "suppliers_toMerge.rename(columns = {'gvkey': 'supplier_gvkey'},inplace = True)    \n",
    "\n",
    "print(suppliers_toMerge.columns)\n",
    "\n",
    "\n",
    "'''suppliers_toMerge = suppliersWithWeather[['year','qtr','gvkey','tmax_quant_1.0','precip_quant_1.0']].\\\n",
    "    rename(columns = {'gvkey': 'supplier_gvkey',\n",
    "                      'tmax_quant_1.0': 'supplier_tmax_quant_1.0',\n",
    "                      'precip_quant_1.0': 'supplier_precip_quant_1.0'})'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customersWithWeather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(supplierWtdAvgWeather.gvkey.unique()) - set(suppliers_toMerge.gvkey.unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestSuppliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestSuppliersWithWeather = customersWithWeather.merge(largestSuppliers[['year', 'gvkey', 'supplier_gvkey']]).merge(suppliers_toMerge)\n",
    "largestSuppliersWithWeather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestSuppliersWithWeather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestSuppliersWithWeather.to_csv(\"../../data/companyData/largestSuppliersWithWeather.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestSuppliersWithWeather.suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestSuppliersWithWeather = pd.read_csv(\"../../data/companyData/largestSuppliersWithWeather.csv\")\n",
    "largestSuppliersWithWeather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largestSuppliersWithWeather.columns[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales-Weighted Average\n",
    "If a company doesn't have sales-specific information, then assume equal shares. This doesn't happen for too many of the companies, thankfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB = c_links[['year','customer_gvkey','supplier_gvkey','salecs']].\\\n",
    "    merge(custExp).rename(columns = {'customer_gvkey': 'gvkey'}).drop_duplicates()\n",
    "\n",
    "customerDB['salesWeight'] = customerDB.salecs/customerDB.totalExp\n",
    "\n",
    "customerDB.fillna(1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge this with the supplier weather data, and use the sales weights to find a sales-weighted average of the weather conditions for the suppliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantVars = [x for x in suppliersWithWeather.columns if (('year' in x) | \n",
    "                                                 ('qtr' in x) |\n",
    "                                                 ('gvkey' in x) |\n",
    "                                                 ('_' in x)) & \n",
    "                                                ~('roa_lagged' in x) & \n",
    "                                                ~('yearQtr' in x)]\n",
    "\n",
    "suppliers_toMerge = suppliersWithWeather[relevantVars]\n",
    "\n",
    "\n",
    "for colname in suppliers_toMerge.columns[3:]:\n",
    "    suppliers_toMerge.rename(columns = {colname: 'supplier_' + colname}, inplace = True)\n",
    "\n",
    "    \n",
    "suppliers_toMerge.rename(columns = {'gvkey': 'supplier_gvkey'},inplace = True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppliers_toMerge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the supplier weather columns, multiply the variable by the fraction of sales attributable to that relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplierWeather = customerDB[['year','gvkey','supplier_gvkey','salesWeight']].merge(suppliers_toMerge)\n",
    "\n",
    "for col in supplierWeather.columns[5:]:\n",
    "        supplierWeather[col]   = supplierWeather.salesWeight*supplierWeather[col]\n",
    "        \n",
    "        \n",
    "\n",
    "supplierWeather.drop(columns = {'supplier_gvkey','salesWeight'}, inplace = True)\n",
    "\n",
    "\n",
    "print(supplierWeather.head())\n",
    "\n",
    "\n",
    "\n",
    "# [['year','qtr','gvkey','supplier_tmax_quant_1.0','supplier_precip_quant_1.0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplierWtdAvgWeather = supplierWeather.groupby(['year','qtr','gvkey']).sum().reset_index().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplierWtdAvgWeather.gvkey.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the supplier weighted average weather data with the customer data that has weather as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customersWithWeather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtdAvgSuppliers = customersWithWeather.merge(supplierWtdAvgWeather)\n",
    "\n",
    "wtdAvgSuppliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtdAvgSuppliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtdAvgSuppliers.to_csv(\"../../data/companyData/wtdAvgSuppliers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtdAvgSuppliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtdAvgSuppliers.columns[wtdAvgSuppliers.columns.str.contains('Tercile')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
