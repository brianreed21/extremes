{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle as pkl\n",
    "# import en_core_web_md\n",
    "# nlp = en_core_web_md.load()\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "# nltk.download('words')\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import PorterStemmer\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "import gc\n",
    "import dask\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import itertools \n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.dates as dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkText_wordList(wordText):\n",
    "    word = wordText[0]\n",
    "    text = wordText[1]\n",
    "    \n",
    "    matchQ = re.search(rf\"\\b(?=\\w){word}\\b(?!\\w)\",text,re.IGNORECASE)\n",
    "    \n",
    "    \n",
    "    return bool(matchQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTense(sent):\n",
    "    text = word_tokenize(sent)\n",
    "    tagged = pos_tag(text)\n",
    "\n",
    "    tense = {}\n",
    "    tense[\"future\"] = len([word for word in tagged if word[1] in [\"MD\",\"VBF\"]])\n",
    "    tense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n",
    "    tense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]]) \n",
    "\n",
    "    tenseCounts = [tense[\"future\"], tense[\"present\"], tense[\"past\"]]\n",
    "    tenseTypes  = ['future', 'present', 'past']\n",
    "\n",
    "    maxType     = np.argmax(tenseCounts)\n",
    "\n",
    "    percPast    = tense['past'] / (tense['future'] + tense['present'] + tense['past'] + 0.0001)\n",
    "\n",
    "    return(tenseTypes[maxType],tenseCounts, percPast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkMentions(word, df):\n",
    "    word = [word]\n",
    "    \n",
    "    termList = list(product(word, df.text))\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        termMentions = pool.map(checkText_wordList,termList)\n",
    "    \n",
    "    print(sum(termMentions)/(len(termMentions) + 0.0001))    \n",
    "\n",
    "    return(termMentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explodeSents(df):\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        df['textSent'] = pool.map(sent_tokenize,df.text)\n",
    "        \n",
    "    df.drop(['text'], axis=1,inplace = True)\n",
    "    df         = df.explode('textSent').reset_index()\n",
    "    df.rename(columns = {'textSent': 'text'}, inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2008\n",
    "\n",
    "fileName = '../../data/filings/8k/cleanedFilings_' + str(year) + '.csv'\n",
    "\n",
    "tempFiles = pd.read_csv(fileName).drop(columns = {'Unnamed: 0'})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term = \"rainfall\"\n",
    "term = \"heat wave\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "##################\n",
    "# 1. Filter the filings down to ones that mention extreme or severe weather\n",
    "\n",
    "tempFiles['extremeWeather'] = checkMentions(term, tempFiles)\n",
    "\n",
    "# don't currently have the accession number so we'll filter on a company-date combination\n",
    "tempFiles['companyDay'] = tempFiles.cik.astype('str') + '_' + tempFiles.fdate.astype('str')\n",
    "\n",
    "\n",
    "tempFiles['weatherKey'] = tempFiles.extremeWeather \n",
    "hasTerm = tempFiles[tempFiles['weatherKey']].reset_index(drop = True)\n",
    "\n",
    "print(time.time() - start)\n",
    "print('done with 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# 2. Explode the filings that mention extreme or severe weather so we can process the individual  \n",
    "hasTerm = explodeSents(hasTerm)\n",
    "\n",
    "extremeSent  = checkMentions(term, hasTerm)\n",
    "\n",
    "\n",
    "sentHasTerm = hasTerm[extremeSent]\n",
    "\n",
    "\n",
    "sentHasTerm = sentHasTerm[['fdate', 'cik', 'nitem','text']].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "print(time.time() - start)\n",
    "print('done with 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentHasTerm.text:\n",
    "    print(sent, \"*********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasTerm = explodeSents(hasTerm)\n",
    "\n",
    "extremeSent  = checkMentions('extreme weather',hasTerm)\n",
    "'''severeSent   = checkMentions('severe weather',hasTerm)\n",
    "\n",
    "weatherSent = np.max([extremeSent,severeSent], axis = 0)'''\n",
    "\n",
    "\n",
    "sentHasTerm = hasTerm[weatherSent]\n",
    "\n",
    "\n",
    "sentHasTerm = sentHasTerm[['fdate', 'cik', 'nitem','text']].drop_duplicates().reset_index(drop = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
